{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bb58342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_scaled.shape: (62926, 82)\n",
      "y_train_scaled.shape: (62926, 63)\n"
     ]
    }
   ],
   "source": [
    "# データの準備\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# トレーニングデータ\n",
    "# data_pairs = [\n",
    "#     ('./data/20250518test3/Opti-track/3_final/Take 2024-11-15 03.19.59 PM.csv',\n",
    "#      './data/20250518test3/insoleSensor/3_final/20241115_152500_left.csv',\n",
    "#      './data/20250518test3/insoleSensor/3_final/20241115_152500_right.csv'),\n",
    "#     ('./data/20250518test3/Opti-track/3_final/Take 2024-11-15 03.26.00 PM.csv',\n",
    "#      './data/20250518test3/insoleSensor/3_final/20241115_153100_left.csv', \n",
    "#      './data/20250518test3/insoleSensor/3_final/20241115_153100_right.csv'),\n",
    "#     ('./data/20250518test3/Opti-track/3_final/Take 2024-11-15 03.31.59 PM.csv', \n",
    "#      './data/20250518test3/insoleSensor/3_final/20241115_153700_left.csv', \n",
    "#      './data/20250518test3/insoleSensor/3_final/20241115_153700_right.csv'),\n",
    "#     ('./data/20250518test3/Opti-track/3_final/Take 2024-11-15 03.37.59 PM.csv', \n",
    "#      './data/20250518test3/insoleSensor/3_final/20241115_154300_left.csv', \n",
    "#      './data/20250518test3/insoleSensor/3_final/20241115_154300_right.csv'),\n",
    "#     ('./data/20250518test3/Opti-track/3_final/Take 2024-11-15 03.43.59 PM.csv', \n",
    "#      './data/20250518test3/insoleSensor/3_final/20241115_154900_left.csv', \n",
    "#      './data/20250518test3/insoleSensor/3_final/20241115_154900_right.csv'),\n",
    "#     ('./data/20250518test4/Opti-track/3_final/Take 2024-12-12 03.06.59 PM.csv',\n",
    "#      './data/20250518test4/insoleSensor/3_final/20241212_152700_left.csv', \n",
    "#      './data/20250518test4/insoleSensor/3_final/20241212_152700_right.csv'),\n",
    "#     ('./data/20250518test4/Opti-track/3_final/Take 2024-12-12 03.45.00 PM.csv', \n",
    "#      './data/20250518test4/insoleSensor/3_final/20241212_160501_left.csv', \n",
    "#      './data/20250518test4/insoleSensor/3_final/20241212_160501_right.csv'),\n",
    "#     ('./data/20250518test4/Opti-track/3_final/Take 2024-12-12 04.28.00 PM.csv', \n",
    "#      './data/20250518test4/insoleSensor/3_final/20241212_164800_left.csv', \n",
    "#      './data/20250518test4/insoleSensor/3_final/20241212_164800_right.csv'),\n",
    "#     ('./data/20250518test4/Opti-track/3_final/Take 2024-12-12 05.17.59 PM.csv', \n",
    "#      './data/20250518test4/insoleSensor/3_final/20241212_173800_left.csv', \n",
    "#      './data/20250518test4/insoleSensor/3_final/20241212_173800_right.csv')\n",
    "# ]\n",
    "\n",
    "# # テストデータ\n",
    "# test_data = ('./data/20250518test3/Opti-track/3_final/Take 2024-11-15 03.49.59 PM.csv', \n",
    "#              './data/20250518test3/insoleSensor/3_final/20241115_155500_left.csv', \n",
    "#              './data/20250518test3/insoleSensor/3_final/20241115_155500_right.csv')\n",
    "\n",
    "data_pairs = [\n",
    "        #\n",
    "        # 第三回収集データ\n",
    "        #\n",
    "        # # 立ちっぱなし\n",
    "        ('./data/20250517old_data/20241115test3/Opti-track/Take 2024-11-15 03.20.00 PM.csv',\n",
    "         './data/20250517old_data/20241115test3/insoleSensor/20241115_152500_left.csv',\n",
    "         './data/20250517old_data/20241115test3/insoleSensor/20241115_152500_right.csv'),\n",
    "        # お辞儀\n",
    "        ('./data/20250517old_data/20241115test3/Opti-track/Take 2024-11-15 03.26.00 PM.csv',\n",
    "         './data/20250517old_data/20241115test3/insoleSensor/20241115_153100_left.csv',\n",
    "         './data/20250517old_data/20241115test3/insoleSensor/20241115_153100_right.csv'),\n",
    "        # 体の横の傾け\n",
    "        ('./data/20250517old_data/20241115test3/Opti-track/Take 2024-11-15 03.32.00 PM.csv',\n",
    "         './data/20250517old_data/20241115test3/insoleSensor/20241115_153700_left.csv',\n",
    "         './data/20250517old_data/20241115test3/insoleSensor/20241115_153700_right.csv'),\n",
    "        # 立つ座る\n",
    "        ('./data/20250517old_data/20241115test3/Opti-track/Take 2024-11-15 03.38.00 PM.csv',\n",
    "         './data/20250517old_data/20241115test3/insoleSensor/20241115_154300_left.csv',\n",
    "         './data/20250517old_data/20241115test3/insoleSensor/20241115_154300_right.csv'),\n",
    "        # スクワット\n",
    "        ('./data/20250517old_data/20241115test3/Opti-track/Take 2024-11-15 03.44.00 PM.csv',\n",
    "         './data/20250517old_data/20241115test3/insoleSensor/20241115_154900_left.csv',\n",
    "         './data/20250517old_data/20241115test3/insoleSensor/20241115_154900_right.csv'),\n",
    "        # 総合(test3)\n",
    "        # ('./data/20241115test3/Opti-track/Take 2024-11-15 03.50.00 PM.csv',\n",
    "        # './data/20241115test3/insoleSensor/20241115_155500_left.csv', \n",
    "        # './data/20241115test3/insoleSensor/20241115_155500_right.csv'),\n",
    "\n",
    "        # 釘宮くん\n",
    "        ('./data/20250517old_data/20241212test4/Opti-track/Take 2024-12-12 03.06.59 PM.csv',\n",
    "         './data/20250517old_data/20241212test4/insoleSensor/20241212_152700_left.csv',\n",
    "         './data/20250517old_data/20241212test4/insoleSensor/20241212_152700_right.csv'),\n",
    "        # 百田くん\n",
    "        ('./data/20250517old_data/20241212test4/Opti-track/Take 2024-12-12 03.45.00 PM.csv',\n",
    "         './data/20250517old_data/20241212test4/insoleSensor/20241212_160501_left.csv',\n",
    "         './data/20250517old_data/20241212test4/insoleSensor/20241212_160501_right.csv'),\n",
    "        # # # # 渡辺(me)\n",
    "        ('./data/20250517old_data/20241212test4/Opti-track/Take 2024-12-12 04.28.00 PM.csv',\n",
    "         './data/20250517old_data/20241212test4/insoleSensor/20241212_164800_left.csv',\n",
    "         './data/20250517old_data/20241212test4/insoleSensor/20241212_164800_right.csv'),\n",
    "        # にるぱむさん\n",
    "        ('./data/20250517old_data/20241212test4/Opti-track/Take 2024-12-12 05.17.59 PM.csv',\n",
    "         './data/20250517old_data/20241212test4/insoleSensor/20241212_173800_left.csv',\n",
    "         './data/20250517old_data/20241212test4/insoleSensor/20241212_173800_right.csv')\n",
    "    ]\n",
    "\n",
    "# テストデータ\n",
    "test_data = ('./data/20250517old_data/20241115test3/Opti-track/Take 2024-11-15 03.50.00 PM.csv', \n",
    "             './data/20250517old_data/20241115test3/insoleSensor/20241115_155500_left.csv', \n",
    "             './data/20250517old_data/20241115test3/insoleSensor/20241115_155500_right.csv')\n",
    "\n",
    "# トレーニングデータを格納するリスト\n",
    "X_train_list = []\n",
    "y_train_list = []\n",
    "\n",
    "for opti_path, left_path, right_path in data_pairs:\n",
    "    left_df = pd.read_csv(left_path)\n",
    "    right_df = pd.read_csv(right_path)\n",
    "    skeleton_df = pd.read_csv(opti_path)\n",
    "\n",
    "    left_df = left_df.fillna(0.0)\n",
    "    right_df = right_df.fillna(0.0)\n",
    "    skeleton_df = skeleton_df.fillna(0.0)\n",
    "\n",
    "    X = pd.concat([left_df, right_df], axis=1).values\n",
    "    y = skeleton_df.values\n",
    "\n",
    "    X_train_list.append(X)\n",
    "    y_train_list.append(y)\n",
    "\n",
    "# リスト内のデータを結合\n",
    "X_train = np.concatenate(X_train_list, axis=0)\n",
    "y_train = np.concatenate(y_train_list, axis=0)\n",
    "\n",
    "# スケーリング\n",
    "x_scaler = StandardScaler()\n",
    "X_train_scaled = x_scaler.fit_transform(X_train)\n",
    "\n",
    "y_scaler = StandardScaler()\n",
    "y_train_scaled = y_scaler.fit_transform(y_train)\n",
    "\n",
    "\n",
    "print(\"X_train_scaled.shape:\", X_train_scaled.shape)\n",
    "print(\"y_train_scaled.shape:\", y_train_scaled.shape)\n",
    "\n",
    "\n",
    "# テストデータの読み込み\n",
    "opti_path_test, left_path_test, right_path_test = test_data\n",
    "left_df_test = pd.read_csv(left_path_test)\n",
    "right_df_test = pd.read_csv(right_path_test)\n",
    "skeleton_df_test = pd.read_csv(opti_path_test)\n",
    "\n",
    "left_df_test = left_df_test.fillna(0.0)\n",
    "right_df_test = right_df_test.fillna(0.0)\n",
    "skeleton_df_test = skeleton_df_test.fillna(0.0)\n",
    "\n",
    "X_test = pd.concat([left_df_test, right_df_test], axis=1).values\n",
    "y_test = skeleton_df_test.values\n",
    "\n",
    "# スケーリング (トレーニングデータでfitしたスケーラーを使用)\n",
    "X_test_scaled = x_scaler.transform(X_test)\n",
    "y_test_scaled = y_scaler.transform(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "996abee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DetasetとDataLoaderの定義\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class PostureDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_dataset = PostureDataset(X_train_scaled, y_train_scaled)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "test_dataset = PostureDataset(X_test_scaled, y_test_scaled)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# バリデーションデータの DataLoader は不要になるので削除またはコメントアウト\n",
    "# val_dataset = PostureDataset(X_val, y_val)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ada49c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\20250415P2P-Insole\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# モデル構築\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel, BertConfig\n",
    "\n",
    "class LIMUBERTRegressor(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim=63, hidden_dim=256, dropout=0.1):\n",
    "        super(LIMUBERTRegressor, self).__init__()\n",
    "\n",
    "        # 入力特徴量をBERTのhidden_sizeにマッピング\n",
    "        self.embedding = nn.Linear(input_dim, hidden_dim)\n",
    "\n",
    "        # BERTの設定\n",
    "        config = BertConfig(\n",
    "            hidden_size=hidden_dim,\n",
    "            num_attention_heads=8,\n",
    "            num_hidden_layers=4,\n",
    "            intermediate_size=hidden_dim * 4,\n",
    "            hidden_dropout_prob=dropout,\n",
    "            attention_probs_dropout_prob=dropout\n",
    "        )\n",
    "        self.bert = BertModel(config)\n",
    "\n",
    "        # 出力層（回帰）\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, output_dim)  # 63次元の骨格ベクトル\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, input_dim] → BERTに合わせて [batch_size, 1, hidden_dim]\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.embedding(x)\n",
    "        attention_mask = torch.ones(x.shape[:-1], dtype=torch.long, device=x.device)\n",
    "\n",
    "        # BERTの順伝播\n",
    "        outputs = self.bert(inputs_embeds=x, attention_mask=attention_mask)\n",
    "        cls_representation = outputs.last_hidden_state[:, 0]  # [CLS] トークン相当\n",
    "\n",
    "        return self.regressor(cls_representation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00bec43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデル・損失関数・最適化の定義\n",
    "model = LIMUBERTRegressor(input_dim=82, output_dim=63)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05b16241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.7120, Test Loss = 0.4358\n",
      "Epoch 2: Train Loss = 0.5316, Test Loss = 0.3745\n",
      "Epoch 3: Train Loss = 0.4665, Test Loss = 0.3810\n",
      "Epoch 4: Train Loss = 0.4296, Test Loss = 0.3534\n",
      "Epoch 5: Train Loss = 0.4047, Test Loss = 0.3426\n",
      "Epoch 6: Train Loss = 0.3854, Test Loss = 0.3593\n",
      "Epoch 7: Train Loss = 0.3714, Test Loss = 0.3588\n",
      "Epoch 8: Train Loss = 0.3588, Test Loss = 0.3453\n",
      "Epoch 9: Train Loss = 0.3475, Test Loss = 0.3386\n",
      "Epoch 10: Train Loss = 0.3375, Test Loss = 0.3472\n",
      "Epoch 11: Train Loss = 0.3307, Test Loss = 0.3850\n",
      "Epoch 12: Train Loss = 0.3215, Test Loss = 0.3303\n",
      "Epoch 13: Train Loss = 0.3154, Test Loss = 0.3312\n",
      "Epoch 14: Train Loss = 0.3088, Test Loss = 0.3533\n",
      "Epoch 15: Train Loss = 0.3033, Test Loss = 0.3488\n",
      "Epoch 16: Train Loss = 0.2973, Test Loss = 0.3530\n",
      "Epoch 17: Train Loss = 0.2921, Test Loss = 0.3401\n",
      "Epoch 18: Train Loss = 0.2879, Test Loss = 0.3418\n",
      "Epoch 19: Train Loss = 0.2820, Test Loss = 0.3488\n",
      "Epoch 20: Train Loss = 0.2774, Test Loss = 0.3409\n",
      "Epoch 21: Train Loss = 0.2734, Test Loss = 0.3612\n",
      "Epoch 22: Train Loss = 0.2698, Test Loss = 0.3341\n",
      "Epoch 23: Train Loss = 0.2661, Test Loss = 0.3469\n",
      "Epoch 24: Train Loss = 0.2615, Test Loss = 0.3594\n",
      "Epoch 25: Train Loss = 0.2577, Test Loss = 0.3368\n",
      "Epoch 26: Train Loss = 0.2545, Test Loss = 0.3383\n",
      "Epoch 27: Train Loss = 0.2523, Test Loss = 0.3377\n",
      "Epoch 28: Train Loss = 0.2482, Test Loss = 0.3360\n",
      "Epoch 29: Train Loss = 0.2455, Test Loss = 0.3389\n",
      "Epoch 30: Train Loss = 0.2431, Test Loss = 0.3449\n",
      "Epoch 31: Train Loss = 0.2403, Test Loss = 0.3198\n",
      "Epoch 32: Train Loss = 0.2371, Test Loss = 0.3249\n",
      "Epoch 33: Train Loss = 0.2338, Test Loss = 0.3241\n",
      "Epoch 34: Train Loss = 0.2315, Test Loss = 0.3321\n",
      "Epoch 35: Train Loss = 0.2290, Test Loss = 0.3283\n",
      "Epoch 36: Train Loss = 0.2278, Test Loss = 0.3307\n",
      "Epoch 37: Train Loss = 0.2246, Test Loss = 0.3292\n",
      "Epoch 38: Train Loss = 0.2234, Test Loss = 0.3265\n",
      "Epoch 39: Train Loss = 0.2206, Test Loss = 0.3380\n",
      "Epoch 40: Train Loss = 0.2187, Test Loss = 0.3416\n",
      "Epoch 41: Train Loss = 0.2160, Test Loss = 0.3327\n",
      "Epoch 42: Train Loss = 0.2141, Test Loss = 0.3417\n",
      "Epoch 43: Train Loss = 0.2129, Test Loss = 0.3277\n",
      "Epoch 44: Train Loss = 0.2103, Test Loss = 0.3281\n",
      "Epoch 45: Train Loss = 0.2089, Test Loss = 0.3346\n",
      "Epoch 46: Train Loss = 0.2072, Test Loss = 0.3322\n",
      "Epoch 47: Train Loss = 0.2052, Test Loss = 0.3244\n",
      "Epoch 48: Train Loss = 0.2040, Test Loss = 0.3153\n",
      "Epoch 49: Train Loss = 0.2021, Test Loss = 0.3163\n",
      "Epoch 50: Train Loss = 0.2005, Test Loss = 0.3453\n"
     ]
    }
   ],
   "source": [
    "# 学習ループ\n",
    "epochs = 30\n",
    "model = LIMUBERTRegressor(input_dim=82, output_dim=63).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)  # 安全な低学習率\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(x_batch)\n",
    "        loss = criterion(preds, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # テストデータでの評価\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in test_loader:\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            preds = model(x_batch)\n",
    "            loss = criterion(preds, y_batch)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Train Loss = {train_loss/len(train_loader):.4f}, \"\n",
    "          f\"Test Loss = {test_loss/len(test_loader):.4f}\")\n",
    "    \n",
    "# モデル保存\n",
    "torch.save(model.state_dict(), \"weight_BERT/bert_pose_regressor.pth\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8a11d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(62895, 3)\n"
     ]
    }
   ],
   "source": [
    "# 推論 (LIMUBERTRegressor が埋め込み済み入力を想定する場合)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # テストデータ全体をTensorに変換\n",
    "    sample_input = torch.tensor(X_test_scaled, dtype=torch.float32).to(device)\n",
    "    predicted_poses = []\n",
    "\n",
    "    for i in range(sample_input.shape[0]):\n",
    "        input_sequence = sample_input[i:i+1] # (1, input_dim)\n",
    "        predicted_pose = model(input_sequence).cpu().numpy()\n",
    "        predicted_poses.append(predicted_pose)\n",
    "\n",
    "    # 予測結果を結合\n",
    "    predicted_pose_original_scale = np.concatenate(predicted_poses, axis=0)\n",
    "    predicted_pose_reshaped = y_scaler.inverse_transform(predicted_pose_original_scale).reshape(-1, 3)\n",
    "    print(predicted_pose_reshaped.shape)\n",
    "\n",
    "    # 結合: shape → (N, 63)\n",
    "    predicted_pose_original_scale = np.concatenate(predicted_poses, axis=0)\n",
    "    predicted_pose_original_scale = y_scaler.inverse_transform(predicted_pose_original_scale)\n",
    "\n",
    "    # 列名作成（21関節 × XYZ = 63列）\n",
    "    column_names = [f'joint_{i}_{coord}' for i in range(21) for coord in ['x', 'y', 'z']]\n",
    "\n",
    "    # DataFrameとして保存（shape: N × 63）\n",
    "    output_df = pd.DataFrame(predicted_pose_original_scale, columns=column_names)\n",
    "    output_df.to_csv(\"output/BERT_predicted_skeleton.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
