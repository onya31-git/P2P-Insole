{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ad972721",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "def add_gaussian_noise(data, noise_factor=0.05):\n",
    "    \"\"\"给数据添加高斯噪声，默认噪声方差为原始数据的5%\"\"\"\n",
    "    std = torch.std(data, dim=0, keepdim=True)  # 计算原始数据的标准差\n",
    "    noise = torch.randn_like(data) * (std * noise_factor)  # 生成噪声\n",
    "    return data + noise\n",
    "\n",
    "\n",
    "def positional_encoding(seq_len, d_model):\n",
    "    # 初始化一个全零矩阵，形状为 (seq_len, d_model)\n",
    "    pe = torch.zeros(seq_len, d_model)\n",
    "\n",
    "    # position: [0, 1, 2, ..., seq_len-1]，形状为 (seq_len, 1)\n",
    "    position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)\n",
    "\n",
    "    # div_term: 指数衰减因子，用于控制正弦和余弦的频率\n",
    "    # torch.arange(0, d_model, 2): 取嵌入维度中偶数位置的索引\n",
    "    div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float) * (-math.log(10000.0) / d_model))\n",
    "\n",
    "    # 偶数位置使用正弦函数，奇数位置使用余弦函数\n",
    "    pe[:, 0::2] = torch.sin(position * div_term)\n",
    "    pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "    return pe\n",
    "\n",
    "\n",
    "def add_positional_encoding(x):\n",
    "    batch_size, seq_len, d_model = x.shape\n",
    "\n",
    "    # 构建位置编码矩阵，形状 (seq_len, d_model)\n",
    "    pe = torch.zeros(seq_len, d_model, device=x.device, dtype=x.dtype)\n",
    "\n",
    "    # 位置向量 (seq_len, 1)\n",
    "    position = torch.arange(0, seq_len, device=x.device, dtype=x.dtype).unsqueeze(1)\n",
    "\n",
    "    # 频率因子（对偶数索引维度计算）\n",
    "    div_term = torch.exp(torch.arange(0, d_model, 2, device=x.device, dtype=x.dtype) * (-math.log(10000.0) / d_model))\n",
    "\n",
    "    # 对偶数维度使用 sin, 奇数维度使用 cos\n",
    "    pe[:, 0::2] = torch.sin(position * div_term)\n",
    "    pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "    # 将位置编码添加到输入张量上，pe.unsqueeze(0) 的形状为 (1, seq_len, d_model)\n",
    "    x = x + pe.unsqueeze(0)\n",
    "    return x\n",
    "\n",
    "\n",
    "class PressureSkeletonDataset(Dataset):\n",
    "    def __init__(self, pressure_data, skeleton_data, decoder_input):\n",
    "        self.pressure_data = torch.FloatTensor(pressure_data)\n",
    "        self.skeleton_data = torch.FloatTensor(skeleton_data)\n",
    "        self.decoder_input = torch.FloatTensor(decoder_input)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pressure_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.pressure_data[idx], self.skeleton_data[idx], self.decoder_input[idx]\n",
    "\n",
    "\n",
    "class EnhancedSkeletonTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, d_model, nhead, num_encoder_layers, num_joints, num_dims=3, dropout=0.1, seq_length=3,\n",
    "                 window_size=5):\n",
    "        super().__init__()\n",
    "\n",
    "        # クラス属性としてnum_jointsを保存\n",
    "        self.num_joints = num_joints\n",
    "        self.num_dims = num_dims\n",
    "        self.seq_length = seq_length\n",
    "        self.window_size = window_size\n",
    "\n",
    "        # 入力の特徴抽出を強化\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Linear(input_dim, d_model)\n",
    "        )\n",
    "\n",
    "        # より深いTransformerネットワーク\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=d_model * 4,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "            norm_first=True\n",
    "        )\n",
    "\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            encoder_layer,\n",
    "            num_layers=num_encoder_layers\n",
    "        )\n",
    "\n",
    "        self.decoder_feature_extractor = nn.Sequential(\n",
    "            nn.Linear(num_joints * num_dims, d_model)\n",
    "        )\n",
    "\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=d_model * 4,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "            norm_first=True\n",
    "        )\n",
    "\n",
    "        self.transformer_decoder = nn.TransformerDecoder(\n",
    "            decoder_layer,\n",
    "            num_layers=num_encoder_layers\n",
    "        )\n",
    "        self.predict = nn.Sequential(\n",
    "            nn.Linear(d_model * seq_length, d_model * window_size)\n",
    "        )\n",
    "\n",
    "        # 出力層の強化\n",
    "        self.output_decoder = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model * 2),\n",
    "            nn.LayerNorm(d_model * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model * 2, d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model, num_joints * num_dims)\n",
    "        )\n",
    "\n",
    "        self.constraint = SkeletonConstraints(num_joints)\n",
    "        # スケール係数（学習可能パラメータ）\n",
    "        self.output_scale = nn.Parameter(torch.ones(1))\n",
    "\n",
    "    def forward(self, x, decoder_input):\n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        # 特徴抽出\n",
    "        features = self.feature_extractor(x)\n",
    "        features = features.unsqueeze(1)\n",
    "        decoder_input = self.decoder_feature_extractor(decoder_input)\n",
    "        decoder_input = add_positional_encoding(decoder_input)\n",
    "\n",
    "        # Transformer処理\n",
    "        transformer_output = self.transformer_encoder(features)\n",
    "        transformer_output = self.transformer_decoder(decoder_input, transformer_output)\n",
    "        # predict = transformer_output[:,transformer_output.shape[1]-1,:]\n",
    "\n",
    "        predict = transformer_output.reshape(batch_size, -1)\n",
    "        predict_next = self.predict(predict)\n",
    "        predict_next = predict_next.reshape(batch_size, self.window_size, -1)\n",
    "\n",
    "        # 出力生成とスケーリング\n",
    "        output = self.output_decoder(predict_next)\n",
    "        # output = self.constraint(output)\n",
    "        output = output * self.output_scale  # 出力のスケーリング\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, save_path, device):\n",
    "    best_val_loss = float('inf')\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for pressure, skeleton, decoder_input in train_loader:\n",
    "            # データをGPUに移動\n",
    "            pressure = pressure.to(device)\n",
    "            skeleton = skeleton.to(device)\n",
    "            decoder_input = decoder_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            pressure = add_gaussian_noise(pressure, noise_factor=0.1)\n",
    "            decoder_input = add_gaussian_noise(decoder_input, noise_factor=0.1)\n",
    "            if torch.rand(1).item() < 0.95:\n",
    "                decoder_input=torch.zeros_like(decoder_input)\n",
    "            outputs = model(pressure, decoder_input)\n",
    "            loss = criterion(outputs, skeleton)\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for pressure, skeleton, decoder_input in val_loader:\n",
    "                # データをGPUに移動\n",
    "                pressure = pressure.to(device)\n",
    "                skeleton = skeleton.to(device)\n",
    "                decoder_input = decoder_input.to(device)\n",
    "\n",
    "                outputs = model(pressure, decoder_input)\n",
    "                loss = criterion(outputs, skeleton)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        # 平均損失の計算\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "        # スケジューラのステップ\n",
    "        scheduler.step(avg_val_loss)\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "        print(f'Epoch {epoch + 1}')\n",
    "        print(f'Training Loss: {avg_train_loss:.4f}')\n",
    "        print(f'Validation Loss: {avg_val_loss:.4f}')\n",
    "        print(f'Learning Rate: {current_lr:.6f}')\n",
    "\n",
    "        # モデルの保存\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            checkpoint = {\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'best_val_loss': best_val_loss,\n",
    "            }\n",
    "            torch.save(checkpoint, save_path)\n",
    "            print(f'Model saved at epoch {epoch + 1}')\n",
    "\n",
    "        print('-' * 60)\n",
    "\n",
    "\n",
    "# class SkeletonLoss(nn.Module):\n",
    "#     def __init__(self, joint_connections):\n",
    "#         super().__init__()\n",
    "#         self.joint_connections = joint_connections\n",
    "\n",
    "#     def forward(self, pred, target):\n",
    "#         # MSE損失\n",
    "#         mse_loss = F.mse_loss(pred, target)\n",
    "\n",
    "#         # 骨格の長さの一貫性を保つための損失\n",
    "#         bone_length_loss = self.calculate_bone_length_loss(pred, target)\n",
    "\n",
    "#         # 関節角度の制約に関する損失\n",
    "#         angle_loss = self.calculate_angle_loss(pred)\n",
    "\n",
    "#         return mse_loss + 0.1 * bone_length_loss + 0.1 * angle_loss\n",
    "\n",
    "\n",
    "def load_model(model, optimizer, scheduler, checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    best_val_loss = checkpoint['best_val_loss']\n",
    "\n",
    "    return model, optimizer, scheduler, epoch, best_val_loss\n",
    "\n",
    "\n",
    "# モデルの推論\n",
    "def predict(model, pressure_data):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pressure_tensor = torch.FloatTensor(pressure_data)\n",
    "        predictions = model(pressure_tensor)\n",
    "    return predictions.numpy()\n",
    "\n",
    "\n",
    "class SkeletonConstraints(nn.Module):\n",
    "    def __init__(self, num_joints):\n",
    "        super().__init__()\n",
    "        self.num_joints = num_joints\n",
    "\n",
    "        # 定义关节层级（父子关系）\n",
    "        self.joint_hierarchy = [\n",
    "            (0, 1), (1, 2), (2, 3), (3, 4),  # 背骨\n",
    "            (5, 6), (6, 7), (7, 8),  # 腕和肩（示例）\n",
    "            (9, 10), (10, 11), (11, 12), (5, 9),  # 另一侧的腕和肩（示例）\n",
    "            (13, 14), (14, 15), (15, 16),  # 腿的一侧（示例）\n",
    "            (17, 18), (18, 19), (19, 20), (13, 17)  # 另一侧的腿（示例）\n",
    "        ]\n",
    "\n",
    "        # 每条骨骼的标准长度，长度与joint_hierarchy的数量保持一致\n",
    "        self.bone_lengths = nn.Parameter(torch.ones(len(self.joint_hierarchy)), requires_grad=False)\n",
    "\n",
    "    def forward(self, skeleton_pred):\n",
    "        \"\"\"\n",
    "        skeleton_pred: 张量，形状 (batch_size, num_joints*3)\n",
    "        \"\"\"\n",
    "        batch_size = skeleton_pred.shape[0]\n",
    "        skeleton_3d = skeleton_pred.view(batch_size, self.num_joints, 3)\n",
    "\n",
    "        # 初始化约束后的骨架，将根关节位置复制过来（假定索引 0 是根关节）\n",
    "        constrained_skeleton = torch.zeros_like(skeleton_3d)\n",
    "        constrained_skeleton[:, 0] = skeleton_3d[:, 0]\n",
    "\n",
    "        # 遍历关节层级，逐步传播骨骼约束\n",
    "        for idx, (parent, child) in enumerate(self.joint_hierarchy):\n",
    "            # 计算原始骨架中父子关节之间的向量\n",
    "            bone_vector = skeleton_3d[:, child] - skeleton_3d[:, parent]\n",
    "            # 计算骨骼向量的长度，并防止除零\n",
    "            bone_length = torch.norm(bone_vector, dim=1, keepdim=True)\n",
    "            bone_length = torch.clamp(bone_length, min=1e-6)\n",
    "            # 使用该骨骼的标准长度\n",
    "            desired_length = self.bone_lengths[idx]\n",
    "            # 对向量归一化后乘以期望长度\n",
    "            normalized_bone = bone_vector * (desired_length / bone_length)\n",
    "            # 利用父关节在约束骨架中的位置确定子关节的位置\n",
    "            constrained_skeleton[:, child] = constrained_skeleton[:, parent] + normalized_bone\n",
    "\n",
    "        return constrained_skeleton.view(batch_size, -1)\n",
    "\n",
    "def compute_exponential_weights(k, m):\n",
    "    \"\"\"计算指数衰减权重 w_i = exp(-m * i)，并归一化\"\"\"\n",
    "    indices = torch.arange(k)  # 生成 i = 0, 1, ..., k-1\n",
    "    weights = torch.exp(-m * indices)  # 计算 w_i\n",
    "    return weights / weights.sum()  # 归一化，使得所有权重之和为 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9047a278",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_combine_data(file_pairs, seq_length, window_size):\n",
    "    \"\"\"複数のデータセットを読み込んで結合する\"\"\"\n",
    "    all_skeleton_data = []\n",
    "    all_pressure_left = []\n",
    "    all_pressure_right = []\n",
    "    all_decoder_input = []\n",
    "\n",
    "    all_skeleton_label = []\n",
    "    for skeleton_file, left_file, right_file in file_pairs:\n",
    "        skeleton = pd.read_csv(skeleton_file)\n",
    "        left = pd.read_csv(left_file, dtype=float, low_memory=False)\n",
    "        right = pd.read_csv(right_file, dtype=float, low_memory=False)\n",
    "        # データ長を揃える\n",
    "        min_length = min(len(skeleton), len(left), len(right))\n",
    "\n",
    "        num_joints_points = skeleton.shape[1]\n",
    "        decoder_input = np.zeros((min_length, seq_length, num_joints_points))\n",
    "        for i in range(min_length):\n",
    "            for j in range(1, seq_length + 1):\n",
    "                if i - j < 0:\n",
    "                    continue\n",
    "                else:\n",
    "                    decoder_input[i, seq_length - j] = skeleton.iloc[i - j]\n",
    "\n",
    "        skeleton_label = np.zeros((min_length, window_size, num_joints_points))\n",
    "        for i in range(min_length):\n",
    "            for j in range(window_size):\n",
    "                if i + j < min_length:\n",
    "                    skeleton_label[i, j] = skeleton.iloc[i + j]\n",
    "        \n",
    "        all_skeleton_data.append(skeleton.iloc[:min_length])\n",
    "        all_pressure_left.append(left.iloc[:min_length])\n",
    "        all_pressure_right.append(right.iloc[:min_length])\n",
    "        all_decoder_input.append(decoder_input)\n",
    "        all_skeleton_label.append(skeleton_label)\n",
    "        \n",
    "\n",
    "    return (pd.concat(all_skeleton_data, ignore_index=True),\n",
    "            pd.concat(all_pressure_left, ignore_index=True),\n",
    "            pd.concat(all_pressure_right, ignore_index=True),\n",
    "            np.concatenate(all_decoder_input),\n",
    "            np.concatenate(all_skeleton_label))\n",
    "\n",
    "\n",
    "def preprocess_pressure_data(left_data, right_data):\n",
    "    \"\"\"圧力、回転、加速度データの前処理\"\"\"\n",
    "\n",
    "    # 左足データから各種センサー値を抽出\n",
    "    left_pressure = left_data.iloc[:, :35]  # 圧力センサーの列を適切に指定\n",
    "    left_rotation = left_data.iloc[:, 35:38]  # 回転データの列を適切に指定\n",
    "    left_accel = left_data.iloc[:, 38:41]  # 加速度データの列を適切に指定\n",
    "\n",
    "    # 右足データから各種センサー値を抽出\n",
    "    right_pressure = right_data.iloc[:, :35]  # 圧力センサーの列を適切に指定\n",
    "    right_rotation = right_data.iloc[:, 35:38]  # 回転データの列を適切に指定\n",
    "    right_accel = right_data.iloc[:, 38:41]  # 加速度データの列を適切に指定\n",
    "\n",
    "    # データの結合(按列（属性）相拼接)\n",
    "    pressure_combined = pd.concat([left_pressure, right_pressure], axis=1)\n",
    "    rotation_combined = pd.concat([left_rotation, right_rotation], axis=1)\n",
    "    accel_combined = pd.concat([left_accel, right_accel], axis=1)\n",
    "\n",
    "    # NaN値を補正\n",
    "    pressure_combined = pressure_combined.fillna(0.0)\n",
    "    rotation_combined = rotation_combined.fillna(0.0)\n",
    "    accel_combined = accel_combined.fillna(0.0)\n",
    "\n",
    "    print(\"Checking pressure data for NaN or Inf...\")\n",
    "    print(\"Pressure NaN count:\", pressure_combined.isna().sum().sum())\n",
    "    print(\"Pressure Inf count:\", np.isinf(pressure_combined).sum().sum())\n",
    "\n",
    "    # 移動平均フィルタの適用\n",
    "    window_size = 3\n",
    "    pressure_combined = pressure_combined.rolling(window=window_size, center=True).mean()\n",
    "    rotation_combined = rotation_combined.rolling(window=window_size, center=True).mean()\n",
    "    accel_combined = accel_combined.rolling(window=window_size, center=True).mean()\n",
    "\n",
    "    # NaN値を前後の値で補間\n",
    "    pressure_combined = pressure_combined.bfill().ffill()\n",
    "    rotation_combined = rotation_combined.bfill().ffill()\n",
    "    accel_combined = accel_combined.bfill().ffill()\n",
    "\n",
    "    # 正規化と標準化のスケーラー初期化\n",
    "    pressure_normalizer = MinMaxScaler()\n",
    "    rotation_normalizer = MinMaxScaler()\n",
    "    accel_normalizer = MinMaxScaler()\n",
    "\n",
    "    pressure_standardizer = StandardScaler(with_mean=True, with_std=True)\n",
    "    rotation_standardizer = StandardScaler(with_mean=True, with_std=True)\n",
    "    accel_standardizer = StandardScaler(with_mean=True, with_std=True)\n",
    "\n",
    "    # データの正規化と標準化\n",
    "    pressure_processed = pressure_standardizer.fit_transform(\n",
    "        pressure_normalizer.fit_transform(pressure_combined)\n",
    "    )\n",
    "    rotation_processed = rotation_standardizer.fit_transform(\n",
    "        rotation_normalizer.fit_transform(rotation_combined)\n",
    "    )\n",
    "    accel_processed = accel_standardizer.fit_transform(\n",
    "        accel_normalizer.fit_transform(accel_combined)\n",
    "    )\n",
    "\n",
    "    # 1次微分と2次微分の計算\n",
    "    pressure_grad1 = np.gradient(pressure_processed, axis=0)\n",
    "    pressure_grad2 = np.gradient(pressure_grad1, axis=0)\n",
    "\n",
    "    rotation_grad1 = np.gradient(rotation_processed, axis=0)\n",
    "    '''不存在物理意义\n",
    "    rotation_grad2 = np.gradient(rotation_grad1, axis=0)\n",
    "\n",
    "    accel_grad1 = np.gradient(accel_processed, axis=0)\n",
    "    accel_grad2 = np.gradient(accel_grad1, axis=0)\n",
    "    '''\n",
    "\n",
    "    # すべての特徴量を結合\n",
    "    input_features = np.concatenate([\n",
    "        pressure_processed,\n",
    "        # pressure_grad1,\n",
    "        # pressure_grad2,\n",
    "        rotation_processed,\n",
    "        # rotation_grad1,\n",
    "        # rotation_grad2,\n",
    "        accel_processed,\n",
    "        # accel_grad1,\n",
    "        # accel_grad2\n",
    "    ], axis=1)\n",
    "\n",
    "    return input_features, {\n",
    "        'pressure': {\n",
    "            'normalizer': pressure_normalizer,\n",
    "            'standardizer': pressure_standardizer\n",
    "        },\n",
    "        'rotation': {\n",
    "            'normalizer': rotation_normalizer,\n",
    "            'standardizer': rotation_standardizer\n",
    "        },\n",
    "        'accel': {\n",
    "            'normalizer': accel_normalizer,\n",
    "            'standardizer': accel_standardizer\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4029ee6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedSkeletonLoss(nn.Module):\n",
    "    def __init__(self, alpha=1.0, beta=0.1, gamma=0.1, window_size=5):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.window_size = window_size\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        # MSE損失\n",
    "        mse_loss = F.mse_loss(pred, target)\n",
    "        # 変化量の損失\n",
    "        motion_loss = F.mse_loss(\n",
    "            pred[1:] - pred[:-1],\n",
    "            target[1:] - target[:-1]\n",
    "        )\n",
    "\n",
    "        # 加速度の損失\n",
    "        accel_loss = F.mse_loss(\n",
    "            pred[2:] + pred[:-2] - 2 * pred[1:-1],\n",
    "            target[2:] + target[:-2] - 2 * target[1:-1]\n",
    "        )\n",
    "        # 骨骼长度的损失\n",
    "        total_loss = 0.0\n",
    "        batch_size = int(pred.shape[0])\n",
    "        num_joints = int(pred.shape[2] / 3)\n",
    "\n",
    "        predicted = pred.view(batch_size, self.window_size, num_joints, 3)\n",
    "        target = target.view(batch_size, self.window_size, num_joints, 3)\n",
    "        joint_hierarchy = [\n",
    "            (0, 1), (1, 2), (2, 3), (3, 4),  # 背骨\n",
    "            (5, 6), (6, 7), (7, 8),  # 腕和肩（示例）\n",
    "            (9, 10), (10, 11), (11, 12), (5, 9),  # 另一侧的腕和肩（示例）\n",
    "            (13, 14), (14, 15), (15, 16),  # 腿的一侧（示例）\n",
    "            (17, 18), (18, 19), (19, 20), (13, 17)  # 另一侧的腿（示例）\n",
    "        ]\n",
    "        for parent, child in joint_hierarchy:\n",
    "            # 计算预测中对应骨骼的长度\n",
    "            pred_bone = predicted[:, :, parent, :] - predicted[:, :, child, :]\n",
    "            pred_length = torch.norm(pred_bone, dim=1)\n",
    "            # 计算教师数据中对应骨骼的长度\n",
    "            target_bone = target[:, :, parent, :] - target[:, :, child, :]\n",
    "            target_length = torch.norm(target_bone, dim=1)\n",
    "            # 均方误差\n",
    "            total_loss += torch.mean((pred_length - target_length) ** 2)\n",
    "        total_loss = total_loss / len(joint_hierarchy)\n",
    "\n",
    "        return self.alpha * mse_loss / batch_size\n",
    "\n",
    "class EnhancedSkeletonLoss_WithAngleConstrains(nn.Module):\n",
    "    def __init__(self, alpha=1.0, beta=0.1, gamma=0.5, window_size=5):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.window_size = window_size\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        # MSE損失\n",
    "        # 假设 pred 和 target 的原始形状为 [batch_size, window_size * num_joints * 3]\n",
    "        batch_size = int(pred.shape[0])\n",
    "        num_joints = int(pred.shape[2] // 3)\n",
    "\n",
    "        # 重塑为 [batch_size, window_size, num_joints, 3]\n",
    "        pred_reshaped = pred.view(batch_size, self.window_size, num_joints, 3)\n",
    "        target_reshaped = target.view(batch_size, self.window_size, num_joints, 3)\n",
    "\n",
    "        # 定义关节点权重，默认全部为 1.0\n",
    "        joint_weights = torch.tensor(0.2, device=pred.device) * torch.ones(num_joints, device=pred.device)\n",
    "        # 对背骨的关节点（索引 0～4）赋予较高权重\n",
    "        for idx in [0, 1, 2, 3, 4]:\n",
    "            joint_weights[idx] = 2.0\n",
    "        # 对两个腿的关节点（例如：一侧 13～16，另一侧 17～20）赋予较高权重\n",
    "        for idx in [13, 14, 15, 16, 17, 18, 19, 20]:\n",
    "            joint_weights[idx] = 2.0\n",
    "\n",
    "        # 计算加权均方误差\n",
    "        # 先计算每个坐标的平方误差，形状为 [batch_size, window_size, num_joints, 3]\n",
    "        squared_diff = (pred_reshaped - target_reshaped) ** 2\n",
    "        # 对坐标求和得到每个关节的误差，形状为 [batch_size, window_size, num_joints]\n",
    "        squared_diff = squared_diff.sum(dim=-1)\n",
    "        # 将关节点的权重扩展到 [1, 1, num_joints] 后相乘\n",
    "        weighted_squared_diff = squared_diff * joint_weights.view(1, 1, num_joints)\n",
    "        # 平均得到加权的均方误差\n",
    "        mse_loss = weighted_squared_diff.mean()\n",
    "\n",
    "        # 変化量の損失\n",
    "        motion_loss = F.mse_loss(\n",
    "            pred[1:] - pred[:-1],\n",
    "            target[1:] - target[:-1]\n",
    "        )\n",
    "\n",
    "        # 加速度の損失\n",
    "        accel_loss = F.mse_loss(\n",
    "            pred[2:] + pred[:-2] - 2 * pred[1:-1],\n",
    "            target[2:] + target[:-2] - 2 * target[1:-1]\n",
    "        )\n",
    "\n",
    "        eps = 1e-6\n",
    "        angle_loss = 0.0\n",
    "        angle_pairs = [\n",
    "            ((0, 1), (1, 2)),\n",
    "            ((1, 2), (2, 3)),\n",
    "            ((2, 3), (3, 4)),\n",
    "            ((13, 17), (17, 18)),\n",
    "            ((13, 17), (13, 14)),\n",
    "            ((17, 18), (18, 19)),\n",
    "            ((18, 19), (19, 20)),\n",
    "            ((13, 14), (14, 15)),\n",
    "            ((14, 15), (15, 16))\n",
    "        ]\n",
    "        for (bone1, bone2) in angle_pairs:\n",
    "            # 预测向量计算\n",
    "            pred_vec1 = pred_reshaped[:, :, bone1[1], :] - pred_reshaped[:, :, bone1[0], :]\n",
    "            pred_vec2 = pred_reshaped[:, :, bone2[1], :] - pred_reshaped[:, :, bone2[0], :]\n",
    "            dot_pred = (pred_vec1 * pred_vec2).sum(dim=-1)\n",
    "            norm_pred1 = torch.norm(pred_vec1, dim=-1)\n",
    "            norm_pred2 = torch.norm(pred_vec2, dim=-1)\n",
    "            cos_pred = dot_pred / (norm_pred1 * norm_pred2 + eps)\n",
    "            cos_pred = torch.clamp(cos_pred, -1.0, 1.0)\n",
    "\n",
    "            # 目标向量计算\n",
    "            target_vec1 = target_reshaped[:, :, bone1[1], :] - target_reshaped[:, :, bone1[0], :]\n",
    "            target_vec2 = target_reshaped[:, :, bone2[1], :] - target_reshaped[:, :, bone2[0], :]\n",
    "            dot_target = (target_vec1 * target_vec2).sum(dim=-1)\n",
    "            norm_target1 = torch.norm(target_vec1, dim=-1)\n",
    "            norm_target2 = torch.norm(target_vec2, dim=-1)\n",
    "            cos_target = dot_target / (norm_target1 * norm_target2 + eps)\n",
    "            cos_target = torch.clamp(cos_target, -1.0, 1.0)\n",
    "\n",
    "            # 直接比较余弦值的差异\n",
    "            angle_loss += F.mse_loss(cos_pred, cos_target)\n",
    "        angle_loss = angle_loss / len(angle_pairs)\n",
    "\n",
    "        # 骨骼长度的损失\n",
    "        total_loss = 0.0\n",
    "\n",
    "        predicted = pred.view(batch_size, self.window_size, num_joints, 3)\n",
    "        target = target.view(batch_size, self.window_size, num_joints, 3)\n",
    "        joint_hierarchy = [\n",
    "            (0, 1), (1, 2), (2, 3), (3, 4),  # 背骨\n",
    "            (5, 6), (6, 7), (7, 8),  # 腕和肩（示例）\n",
    "            (9, 10), (10, 11), (11, 12), (5, 9),  # 另一侧的腕和肩（示例）\n",
    "            (13, 14), (14, 15), (15, 16),  # 腿的一侧（示例）\n",
    "            (17, 18), (18, 19), (19, 20), (13, 17)  # 另一侧的腿（示例）\n",
    "        ]\n",
    "        for parent, child in joint_hierarchy:\n",
    "            # 计算预测中对应骨骼的长度\n",
    "            pred_bone = predicted[:, :, parent, :] - predicted[:, :, child, :]\n",
    "            pred_length = torch.norm(pred_bone, dim=1)\n",
    "            # 计算教师数据中对应骨骼的长度\n",
    "            target_bone = target[:, :, parent, :] - target[:, :, child, :]\n",
    "            target_length = torch.norm(target_bone, dim=1)\n",
    "            # 均方误差\n",
    "            total_loss += torch.mean((pred_length - target_length) ** 2)\n",
    "        total_loss = total_loss / len(joint_hierarchy)\n",
    "\n",
    "        # print(mse_loss,(motion_loss + accel_loss),angle_loss)\n",
    "        return self.alpha * mse_loss + 0 * self.beta * (motion_loss + accel_loss) + self.gamma * angle_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c6f5d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking pressure data for NaN or Inf...\n",
      "Pressure NaN count: 0\n",
      "Pressure Inf count: 0\n",
      "(62926, 82)\n",
      "(50340, 3, 63)\n",
      "(12586, 3, 63)\n",
      "[[ -14.99307909  881.8978575    -3.11338427    0.          950.\n",
      "     0.          102.50592027 1123.691498      9.71420833  158.63968028\n",
      "  1348.022247     15.31464261  210.98121645 1488.819427     12.4203218\n",
      "   146.68631394 1288.309112     51.64099829  130.48158898 1290.074494\n",
      "   184.06483266   -6.46598707 1034.561738    188.07936363 -114.8325657\n",
      "   847.7473145   168.88479488  150.37490746 1289.261811    -23.5031767\n",
      "   149.7121561  1281.435395   -156.69973297   39.60113919 1015.912263\n",
      "  -194.5284549   -58.59240998  823.541107   -213.52571135  -14.96663645\n",
      "   877.5903935    90.84482729  -71.6038666   497.970734     95.08333744\n",
      "    75.56414382  175.400391     91.28129912  -65.42576587  114.462776\n",
      "    98.61923133  -14.96663645  886.205292    -97.06843785  -43.98468678\n",
      "   503.715698   -111.36629904   95.73768125  178.3403245   -93.3742821\n",
      "   -37.80150672  116.452736   -137.92478274]\n",
      " [ -12.90366917  881.240173     -2.87533963    0.          950.\n",
      "     0.           95.34290042 1127.9974065     9.07267776  141.95059666\n",
      "  1354.9507145    14.48083608  186.04588345 1498.9919735    11.31389872\n",
      "   132.52793165 1294.3767395    50.84256077  116.7037624  1295.8726505\n",
      "   183.30292958   -5.43024441 1033.0122375   190.80518682 -103.18607938\n",
      "   840.2063595   174.28085489  136.04918649 1295.4383845   -24.30815439\n",
      "   135.68437831 1287.6625055  -157.50695738   35.87911008 1018.1913145\n",
      "  -195.99098174  -56.43761418  822.885742   -214.5673659   -12.90782524\n",
      "   877.286804     91.09754442  -63.87290822  496.8683775    96.1988905\n",
      "    78.47644571  172.1779475    92.11317933  -63.08745054  112.3988115\n",
      "    97.71782953  -12.90782524  885.1935425   -96.84879125  -39.97738854\n",
      "   502.581634   -111.70216586   98.25929477  176.5366895   -94.03261807\n",
      "   -34.7527382   114.307655   -139.60401085]\n",
      " [ -11.21736088  880.5063165    -1.97892081    0.          950.\n",
      "     0.           89.5754484  1131.5519715     5.56129168  127.63703742\n",
      "  1360.5386045     9.11236506  163.13146451 1507.2974545     4.10480643\n",
      "   121.80684426 1299.3254085    45.83272023  110.86119198 1300.6258235\n",
      "   178.8026716     2.90964612 1031.9198305   192.93609107  -85.74641507\n",
      "   834.419403    180.91262871  122.57046726 1300.3649595   -29.3962382\n",
      "   117.53892691 1292.4982605  -162.49510672   26.81769808 1019.3052055\n",
      "  -197.03885716  -59.92287924  821.0842285  -211.04907067  -11.1698835\n",
      "   877.8372495    92.0439793   -47.47702535  495.744781     98.05088111\n",
      "    86.68113012  167.6508025    90.33895523  -55.11084659  108.8368375\n",
      "    99.22124708  -11.1698835   883.175384    -95.99763491  -33.97475941\n",
      "   500.2041925  -109.60063965  101.25264938  172.7092355   -97.67216521\n",
      "   -33.025644    110.6422345  -139.62149465]]\n",
      "Using device: cuda:0\n",
      "Checking final training and validation data...\n",
      "Train input NaN count: 0 Inf count: 0\n",
      "Train skeleton NaN count: 0 Inf count: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\20250415P2P-Insole\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:382: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Training Loss: 920862.6862\n",
      "Validation Loss: 711141.8125\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 1\n",
      "------------------------------------------------------------\n",
      "Epoch 2\n",
      "Training Loss: 462890.1632\n",
      "Validation Loss: 226237.2748\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 2\n",
      "------------------------------------------------------------\n",
      "Epoch 3\n",
      "Training Loss: 93678.6637\n",
      "Validation Loss: 36697.8839\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 3\n",
      "------------------------------------------------------------\n",
      "Epoch 4\n",
      "Training Loss: 32570.5960\n",
      "Validation Loss: 26143.9355\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 4\n",
      "------------------------------------------------------------\n",
      "Epoch 5\n",
      "Training Loss: 27604.8691\n",
      "Validation Loss: 24518.2100\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 5\n",
      "------------------------------------------------------------\n",
      "Epoch 6\n",
      "Training Loss: 23740.9748\n",
      "Validation Loss: 21082.7497\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 6\n",
      "------------------------------------------------------------\n",
      "Epoch 7\n",
      "Training Loss: 20958.8606\n",
      "Validation Loss: 18967.5485\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 7\n",
      "------------------------------------------------------------\n",
      "Epoch 8\n",
      "Training Loss: 19345.5741\n",
      "Validation Loss: 17642.1892\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 8\n",
      "------------------------------------------------------------\n",
      "Epoch 9\n",
      "Training Loss: 18744.6452\n",
      "Validation Loss: 17043.2141\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 9\n",
      "------------------------------------------------------------\n",
      "Epoch 10\n",
      "Training Loss: 17761.4070\n",
      "Validation Loss: 15826.6637\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 10\n",
      "------------------------------------------------------------\n",
      "Epoch 11\n",
      "Training Loss: 16463.8586\n",
      "Validation Loss: 14245.9761\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 11\n",
      "------------------------------------------------------------\n",
      "Epoch 12\n",
      "Training Loss: 15157.4457\n",
      "Validation Loss: 13159.2404\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 12\n",
      "------------------------------------------------------------\n",
      "Epoch 13\n",
      "Training Loss: 14347.4572\n",
      "Validation Loss: 12490.2544\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 13\n",
      "------------------------------------------------------------\n",
      "Epoch 14\n",
      "Training Loss: 13816.1014\n",
      "Validation Loss: 12012.8422\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 14\n",
      "------------------------------------------------------------\n",
      "Epoch 15\n",
      "Training Loss: 13628.6217\n",
      "Validation Loss: 11933.4510\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 15\n",
      "------------------------------------------------------------\n",
      "Epoch 16\n",
      "Training Loss: 13340.7000\n",
      "Validation Loss: 11662.8877\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 16\n",
      "------------------------------------------------------------\n",
      "Epoch 17\n",
      "Training Loss: 13133.4861\n",
      "Validation Loss: 11579.3678\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 17\n",
      "------------------------------------------------------------\n",
      "Epoch 18\n",
      "Training Loss: 12945.6390\n",
      "Validation Loss: 11679.7844\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 19\n",
      "Training Loss: 12811.6081\n",
      "Validation Loss: 11435.5072\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 19\n",
      "------------------------------------------------------------\n",
      "Epoch 20\n",
      "Training Loss: 12617.8651\n",
      "Validation Loss: 10985.7317\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 20\n",
      "------------------------------------------------------------\n",
      "Epoch 21\n",
      "Training Loss: 12331.1615\n",
      "Validation Loss: 10951.4235\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 21\n",
      "------------------------------------------------------------\n",
      "Epoch 22\n",
      "Training Loss: 12170.0880\n",
      "Validation Loss: 10290.1960\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 22\n",
      "------------------------------------------------------------\n",
      "Epoch 23\n",
      "Training Loss: 11867.8992\n",
      "Validation Loss: 10064.6095\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 23\n",
      "------------------------------------------------------------\n",
      "Epoch 24\n",
      "Training Loss: 11502.2005\n",
      "Validation Loss: 9551.5911\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 24\n",
      "------------------------------------------------------------\n",
      "Epoch 25\n",
      "Training Loss: 11053.5346\n",
      "Validation Loss: 9157.3128\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 25\n",
      "------------------------------------------------------------\n",
      "Epoch 26\n",
      "Training Loss: 10426.9972\n",
      "Validation Loss: 8452.5150\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 26\n",
      "------------------------------------------------------------\n",
      "Epoch 27\n",
      "Training Loss: 9948.7342\n",
      "Validation Loss: 8070.5506\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 27\n",
      "------------------------------------------------------------\n",
      "Epoch 28\n",
      "Training Loss: 9456.0993\n",
      "Validation Loss: 7857.5823\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 28\n",
      "------------------------------------------------------------\n",
      "Epoch 29\n",
      "Training Loss: 9094.7345\n",
      "Validation Loss: 7141.7713\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 29\n",
      "------------------------------------------------------------\n",
      "Epoch 30\n",
      "Training Loss: 8576.9254\n",
      "Validation Loss: 6597.7191\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 30\n",
      "------------------------------------------------------------\n",
      "Epoch 31\n",
      "Training Loss: 7911.4305\n",
      "Validation Loss: 6010.9670\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 31\n",
      "------------------------------------------------------------\n",
      "Epoch 32\n",
      "Training Loss: 7333.1405\n",
      "Validation Loss: 5328.1301\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 32\n",
      "------------------------------------------------------------\n",
      "Epoch 33\n",
      "Training Loss: 7016.7407\n",
      "Validation Loss: 5085.2777\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 33\n",
      "------------------------------------------------------------\n",
      "Epoch 34\n",
      "Training Loss: 6722.2365\n",
      "Validation Loss: 4946.2138\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 34\n",
      "------------------------------------------------------------\n",
      "Epoch 35\n",
      "Training Loss: 6469.7338\n",
      "Validation Loss: 4557.8506\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 35\n",
      "------------------------------------------------------------\n",
      "Epoch 36\n",
      "Training Loss: 6152.9608\n",
      "Validation Loss: 4309.6048\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 36\n",
      "------------------------------------------------------------\n",
      "Epoch 37\n",
      "Training Loss: 5923.3026\n",
      "Validation Loss: 4116.4444\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 37\n",
      "------------------------------------------------------------\n",
      "Epoch 38\n",
      "Training Loss: 5713.6205\n",
      "Validation Loss: 4019.1304\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 38\n",
      "------------------------------------------------------------\n",
      "Epoch 39\n",
      "Training Loss: 5510.2657\n",
      "Validation Loss: 3679.6072\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 39\n",
      "------------------------------------------------------------\n",
      "Epoch 40\n",
      "Training Loss: 5286.5792\n",
      "Validation Loss: 3473.6272\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 40\n",
      "------------------------------------------------------------\n",
      "Epoch 41\n",
      "Training Loss: 5131.2744\n",
      "Validation Loss: 3364.1766\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 41\n",
      "------------------------------------------------------------\n",
      "Epoch 42\n",
      "Training Loss: 4950.9596\n",
      "Validation Loss: 3339.9355\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 42\n",
      "------------------------------------------------------------\n",
      "Epoch 43\n",
      "Training Loss: 4795.8700\n",
      "Validation Loss: 3346.4457\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 44\n",
      "Training Loss: 4704.8566\n",
      "Validation Loss: 3059.0876\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 44\n",
      "------------------------------------------------------------\n",
      "Epoch 45\n",
      "Training Loss: 4602.2596\n",
      "Validation Loss: 2883.6967\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 45\n",
      "------------------------------------------------------------\n",
      "Epoch 46\n",
      "Training Loss: 4482.9688\n",
      "Validation Loss: 2875.4379\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 46\n",
      "------------------------------------------------------------\n",
      "Epoch 47\n",
      "Training Loss: 4394.6314\n",
      "Validation Loss: 2804.9144\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 47\n",
      "------------------------------------------------------------\n",
      "Epoch 48\n",
      "Training Loss: 4287.1005\n",
      "Validation Loss: 2728.1575\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 48\n",
      "------------------------------------------------------------\n",
      "Epoch 49\n",
      "Training Loss: 4194.3656\n",
      "Validation Loss: 2684.3510\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 49\n",
      "------------------------------------------------------------\n",
      "Epoch 50\n",
      "Training Loss: 4128.1161\n",
      "Validation Loss: 2640.6464\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 50\n",
      "------------------------------------------------------------\n",
      "Epoch 51\n",
      "Training Loss: 4032.9495\n",
      "Validation Loss: 2495.9094\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 51\n",
      "------------------------------------------------------------\n",
      "Epoch 52\n",
      "Training Loss: 3980.7580\n",
      "Validation Loss: 2554.7694\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 53\n",
      "Training Loss: 3896.0117\n",
      "Validation Loss: 2390.1429\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 53\n",
      "------------------------------------------------------------\n",
      "Epoch 54\n",
      "Training Loss: 3845.4123\n",
      "Validation Loss: 2397.7614\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 55\n",
      "Training Loss: 3750.9041\n",
      "Validation Loss: 2292.2634\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 55\n",
      "------------------------------------------------------------\n",
      "Epoch 56\n",
      "Training Loss: 3653.5527\n",
      "Validation Loss: 2211.6600\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 56\n",
      "------------------------------------------------------------\n",
      "Epoch 57\n",
      "Training Loss: 3594.6988\n",
      "Validation Loss: 2191.6681\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 57\n",
      "------------------------------------------------------------\n",
      "Epoch 58\n",
      "Training Loss: 3556.5514\n",
      "Validation Loss: 2105.8351\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 58\n",
      "------------------------------------------------------------\n",
      "Epoch 59\n",
      "Training Loss: 3491.6563\n",
      "Validation Loss: 2206.7183\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 60\n",
      "Training Loss: 3388.7595\n",
      "Validation Loss: 2025.2071\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 60\n",
      "------------------------------------------------------------\n",
      "Epoch 61\n",
      "Training Loss: 3361.6130\n",
      "Validation Loss: 1943.8118\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 61\n",
      "------------------------------------------------------------\n",
      "Epoch 62\n",
      "Training Loss: 3332.9462\n",
      "Validation Loss: 1950.7686\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 63\n",
      "Training Loss: 3262.4846\n",
      "Validation Loss: 1831.6750\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 63\n",
      "------------------------------------------------------------\n",
      "Epoch 64\n",
      "Training Loss: 3209.0824\n",
      "Validation Loss: 1879.9472\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 65\n",
      "Training Loss: 3220.8395\n",
      "Validation Loss: 1812.7815\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 65\n",
      "------------------------------------------------------------\n",
      "Epoch 66\n",
      "Training Loss: 3138.6199\n",
      "Validation Loss: 1854.1790\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 67\n",
      "Training Loss: 3102.6596\n",
      "Validation Loss: 1740.2317\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 67\n",
      "------------------------------------------------------------\n",
      "Epoch 68\n",
      "Training Loss: 3079.6706\n",
      "Validation Loss: 1746.9769\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 69\n",
      "Training Loss: 3011.1520\n",
      "Validation Loss: 1710.2418\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 69\n",
      "------------------------------------------------------------\n",
      "Epoch 70\n",
      "Training Loss: 3001.4233\n",
      "Validation Loss: 1744.0394\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 71\n",
      "Training Loss: 2926.5094\n",
      "Validation Loss: 1688.1175\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 71\n",
      "------------------------------------------------------------\n",
      "Epoch 72\n",
      "Training Loss: 2917.8759\n",
      "Validation Loss: 1676.1742\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 72\n",
      "------------------------------------------------------------\n",
      "Epoch 73\n",
      "Training Loss: 2963.0206\n",
      "Validation Loss: 1790.8006\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 74\n",
      "Training Loss: 2860.5551\n",
      "Validation Loss: 1570.0255\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 74\n",
      "------------------------------------------------------------\n",
      "Epoch 75\n",
      "Training Loss: 2853.7115\n",
      "Validation Loss: 1556.0230\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 75\n",
      "------------------------------------------------------------\n",
      "Epoch 76\n",
      "Training Loss: 2839.9556\n",
      "Validation Loss: 1712.0242\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 77\n",
      "Training Loss: 2831.7058\n",
      "Validation Loss: 1567.3423\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 78\n",
      "Training Loss: 2745.4434\n",
      "Validation Loss: 1525.6929\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 78\n",
      "------------------------------------------------------------\n",
      "Epoch 79\n",
      "Training Loss: 2725.7359\n",
      "Validation Loss: 1494.6170\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 79\n",
      "------------------------------------------------------------\n",
      "Epoch 80\n",
      "Training Loss: 2751.2040\n",
      "Validation Loss: 1485.0948\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 80\n",
      "------------------------------------------------------------\n",
      "Epoch 81\n",
      "Training Loss: 2730.1617\n",
      "Validation Loss: 1482.0569\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 81\n",
      "------------------------------------------------------------\n",
      "Epoch 82\n",
      "Training Loss: 2663.4362\n",
      "Validation Loss: 1588.8014\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 83\n",
      "Training Loss: 2630.8415\n",
      "Validation Loss: 1433.9371\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 83\n",
      "------------------------------------------------------------\n",
      "Epoch 84\n",
      "Training Loss: 2602.3003\n",
      "Validation Loss: 1428.6240\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 84\n",
      "------------------------------------------------------------\n",
      "Epoch 85\n",
      "Training Loss: 2604.8374\n",
      "Validation Loss: 1433.1029\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 86\n",
      "Training Loss: 2601.9381\n",
      "Validation Loss: 1428.0736\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 86\n",
      "------------------------------------------------------------\n",
      "Epoch 87\n",
      "Training Loss: 2541.0718\n",
      "Validation Loss: 1460.4317\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 88\n",
      "Training Loss: 2493.6055\n",
      "Validation Loss: 1360.9959\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 88\n",
      "------------------------------------------------------------\n",
      "Epoch 89\n",
      "Training Loss: 2492.7766\n",
      "Validation Loss: 1310.0091\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 89\n",
      "------------------------------------------------------------\n",
      "Epoch 90\n",
      "Training Loss: 2482.8714\n",
      "Validation Loss: 1332.0530\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 91\n",
      "Training Loss: 2453.7877\n",
      "Validation Loss: 1359.3504\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 92\n",
      "Training Loss: 2421.8111\n",
      "Validation Loss: 1285.0398\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 92\n",
      "------------------------------------------------------------\n",
      "Epoch 93\n",
      "Training Loss: 2393.8228\n",
      "Validation Loss: 1319.8366\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 94\n",
      "Training Loss: 2367.5260\n",
      "Validation Loss: 1451.2884\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 95\n",
      "Training Loss: 2401.6819\n",
      "Validation Loss: 1245.8081\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 95\n",
      "------------------------------------------------------------\n",
      "Epoch 96\n",
      "Training Loss: 2352.2629\n",
      "Validation Loss: 1290.7752\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 97\n",
      "Training Loss: 2355.2203\n",
      "Validation Loss: 1222.9168\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 97\n",
      "------------------------------------------------------------\n",
      "Epoch 98\n",
      "Training Loss: 2333.2718\n",
      "Validation Loss: 1240.8145\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 99\n",
      "Training Loss: 2306.1766\n",
      "Validation Loss: 1261.7276\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 100\n",
      "Training Loss: 2299.6518\n",
      "Validation Loss: 1229.4653\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 101\n",
      "Training Loss: 2279.8842\n",
      "Validation Loss: 1231.0564\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 102\n",
      "Training Loss: 2305.9265\n",
      "Validation Loss: 1244.6789\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 103\n",
      "Training Loss: 2251.0155\n",
      "Validation Loss: 1173.3520\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 103\n",
      "------------------------------------------------------------\n",
      "Epoch 104\n",
      "Training Loss: 2247.8760\n",
      "Validation Loss: 1176.7127\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 105\n",
      "Training Loss: 2196.3652\n",
      "Validation Loss: 1155.7538\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 105\n",
      "------------------------------------------------------------\n",
      "Epoch 106\n",
      "Training Loss: 2174.6183\n",
      "Validation Loss: 1123.4323\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 106\n",
      "------------------------------------------------------------\n",
      "Epoch 107\n",
      "Training Loss: 2197.3066\n",
      "Validation Loss: 1118.7510\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 107\n",
      "------------------------------------------------------------\n",
      "Epoch 108\n",
      "Training Loss: 2149.7064\n",
      "Validation Loss: 1219.0847\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 109\n",
      "Training Loss: 2153.1631\n",
      "Validation Loss: 1105.3850\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 109\n",
      "------------------------------------------------------------\n",
      "Epoch 110\n",
      "Training Loss: 2178.7314\n",
      "Validation Loss: 1102.3627\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 110\n",
      "------------------------------------------------------------\n",
      "Epoch 111\n",
      "Training Loss: 2116.6718\n",
      "Validation Loss: 1095.4388\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 111\n",
      "------------------------------------------------------------\n",
      "Epoch 112\n",
      "Training Loss: 2135.2095\n",
      "Validation Loss: 1118.6705\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 113\n",
      "Training Loss: 2105.3810\n",
      "Validation Loss: 1111.3580\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 114\n",
      "Training Loss: 2089.4947\n",
      "Validation Loss: 1064.0792\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 114\n",
      "------------------------------------------------------------\n",
      "Epoch 115\n",
      "Training Loss: 2090.2161\n",
      "Validation Loss: 1088.4325\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 116\n",
      "Training Loss: 2073.1133\n",
      "Validation Loss: 1132.9555\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 117\n",
      "Training Loss: 2077.7190\n",
      "Validation Loss: 1121.2402\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 118\n",
      "Training Loss: 2081.7133\n",
      "Validation Loss: 1075.5652\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 119\n",
      "Training Loss: 2039.2369\n",
      "Validation Loss: 1377.4938\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 120\n",
      "Training Loss: 2041.8481\n",
      "Validation Loss: 1056.7176\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 120\n",
      "------------------------------------------------------------\n",
      "Epoch 121\n",
      "Training Loss: 2003.4501\n",
      "Validation Loss: 1078.2358\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 122\n",
      "Training Loss: 1996.0469\n",
      "Validation Loss: 1016.8814\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 122\n",
      "------------------------------------------------------------\n",
      "Epoch 123\n",
      "Training Loss: 2032.2505\n",
      "Validation Loss: 1043.4404\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 124\n",
      "Training Loss: 1982.2765\n",
      "Validation Loss: 1010.3969\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 124\n",
      "------------------------------------------------------------\n",
      "Epoch 125\n",
      "Training Loss: 1972.4695\n",
      "Validation Loss: 1017.1815\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 126\n",
      "Training Loss: 1931.8720\n",
      "Validation Loss: 994.8069\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 126\n",
      "------------------------------------------------------------\n",
      "Epoch 127\n",
      "Training Loss: 1949.7556\n",
      "Validation Loss: 998.6659\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 128\n",
      "Training Loss: 1922.1314\n",
      "Validation Loss: 980.2826\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 128\n",
      "------------------------------------------------------------\n",
      "Epoch 129\n",
      "Training Loss: 1924.4852\n",
      "Validation Loss: 996.4116\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 130\n",
      "Training Loss: 1917.4436\n",
      "Validation Loss: 989.7373\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 131\n",
      "Training Loss: 1878.0738\n",
      "Validation Loss: 972.6730\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 131\n",
      "------------------------------------------------------------\n",
      "Epoch 132\n",
      "Training Loss: 1886.2974\n",
      "Validation Loss: 979.7135\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 133\n",
      "Training Loss: 1890.3845\n",
      "Validation Loss: 951.4646\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 133\n",
      "------------------------------------------------------------\n",
      "Epoch 134\n",
      "Training Loss: 1859.5994\n",
      "Validation Loss: 944.7273\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 134\n",
      "------------------------------------------------------------\n",
      "Epoch 135\n",
      "Training Loss: 1853.1126\n",
      "Validation Loss: 962.2653\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 136\n",
      "Training Loss: 1866.5297\n",
      "Validation Loss: 940.5450\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 136\n",
      "------------------------------------------------------------\n",
      "Epoch 137\n",
      "Training Loss: 1851.2642\n",
      "Validation Loss: 976.9253\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 138\n",
      "Training Loss: 1839.3425\n",
      "Validation Loss: 951.1674\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 139\n",
      "Training Loss: 1828.5985\n",
      "Validation Loss: 988.3110\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 140\n",
      "Training Loss: 1825.1862\n",
      "Validation Loss: 931.5447\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 140\n",
      "------------------------------------------------------------\n",
      "Epoch 141\n",
      "Training Loss: 1798.2686\n",
      "Validation Loss: 979.2337\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 142\n",
      "Training Loss: 1788.9010\n",
      "Validation Loss: 1017.7804\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 143\n",
      "Training Loss: 1763.9637\n",
      "Validation Loss: 902.8220\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 143\n",
      "------------------------------------------------------------\n",
      "Epoch 144\n",
      "Training Loss: 1770.0404\n",
      "Validation Loss: 931.2911\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 145\n",
      "Training Loss: 1777.4571\n",
      "Validation Loss: 896.1404\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 145\n",
      "------------------------------------------------------------\n",
      "Epoch 146\n",
      "Training Loss: 1744.8356\n",
      "Validation Loss: 874.0089\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 146\n",
      "------------------------------------------------------------\n",
      "Epoch 147\n",
      "Training Loss: 1780.6744\n",
      "Validation Loss: 892.2876\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 148\n",
      "Training Loss: 1754.7125\n",
      "Validation Loss: 905.3853\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 149\n",
      "Training Loss: 1750.5324\n",
      "Validation Loss: 924.5106\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 150\n",
      "Training Loss: 1736.3976\n",
      "Validation Loss: 885.2614\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 151\n",
      "Training Loss: 1703.4839\n",
      "Validation Loss: 872.7647\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 151\n",
      "------------------------------------------------------------\n",
      "Epoch 152\n",
      "Training Loss: 1715.6490\n",
      "Validation Loss: 849.4705\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 152\n",
      "------------------------------------------------------------\n",
      "Epoch 153\n",
      "Training Loss: 1721.3614\n",
      "Validation Loss: 875.6001\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 154\n",
      "Training Loss: 1684.6531\n",
      "Validation Loss: 838.0336\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 154\n",
      "------------------------------------------------------------\n",
      "Epoch 155\n",
      "Training Loss: 1708.1527\n",
      "Validation Loss: 839.3219\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 156\n",
      "Training Loss: 1690.7767\n",
      "Validation Loss: 829.5766\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 156\n",
      "------------------------------------------------------------\n",
      "Epoch 157\n",
      "Training Loss: 1705.3746\n",
      "Validation Loss: 868.6338\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 158\n",
      "Training Loss: 1646.1357\n",
      "Validation Loss: 1003.0476\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 159\n",
      "Training Loss: 1655.5252\n",
      "Validation Loss: 815.1433\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 159\n",
      "------------------------------------------------------------\n",
      "Epoch 160\n",
      "Training Loss: 1629.6414\n",
      "Validation Loss: 883.2969\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 161\n",
      "Training Loss: 1643.4438\n",
      "Validation Loss: 822.5871\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 162\n",
      "Training Loss: 1612.6624\n",
      "Validation Loss: 809.4906\n",
      "Learning Rate: 0.000100\n",
      "Model saved at epoch 162\n",
      "------------------------------------------------------------\n",
      "Epoch 163\n",
      "Training Loss: 1631.7279\n",
      "Validation Loss: 902.0501\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 164\n",
      "Training Loss: 1601.3473\n",
      "Validation Loss: 842.9287\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 165\n",
      "Training Loss: 1623.1061\n",
      "Validation Loss: 825.2620\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 166\n",
      "Training Loss: 1598.4541\n",
      "Validation Loss: 830.3654\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 167\n",
      "Training Loss: 1612.3664\n",
      "Validation Loss: 825.9365\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 168\n",
      "Training Loss: 1601.1823\n",
      "Validation Loss: 840.1780\n",
      "Learning Rate: 0.000050\n",
      "------------------------------------------------------------\n",
      "Epoch 169\n",
      "Training Loss: 1462.2447\n",
      "Validation Loss: 747.6956\n",
      "Learning Rate: 0.000050\n",
      "Model saved at epoch 169\n",
      "------------------------------------------------------------\n",
      "Epoch 170\n",
      "Training Loss: 1422.5073\n",
      "Validation Loss: 732.4829\n",
      "Learning Rate: 0.000050\n",
      "Model saved at epoch 170\n",
      "------------------------------------------------------------\n",
      "Epoch 171\n",
      "Training Loss: 1402.8173\n",
      "Validation Loss: 730.4464\n",
      "Learning Rate: 0.000050\n",
      "Model saved at epoch 171\n",
      "------------------------------------------------------------\n",
      "Epoch 172\n",
      "Training Loss: 1409.9861\n",
      "Validation Loss: 735.1556\n",
      "Learning Rate: 0.000050\n",
      "------------------------------------------------------------\n",
      "Epoch 173\n",
      "Training Loss: 1397.5843\n",
      "Validation Loss: 735.4017\n",
      "Learning Rate: 0.000050\n",
      "------------------------------------------------------------\n",
      "Epoch 174\n",
      "Training Loss: 1402.5218\n",
      "Validation Loss: 718.7896\n",
      "Learning Rate: 0.000050\n",
      "Model saved at epoch 174\n",
      "------------------------------------------------------------\n",
      "Epoch 175\n",
      "Training Loss: 1378.0281\n",
      "Validation Loss: 720.9664\n",
      "Learning Rate: 0.000050\n",
      "------------------------------------------------------------\n",
      "Epoch 176\n",
      "Training Loss: 1383.1402\n",
      "Validation Loss: 703.4150\n",
      "Learning Rate: 0.000050\n",
      "Model saved at epoch 176\n",
      "------------------------------------------------------------\n",
      "Epoch 177\n",
      "Training Loss: 1367.4109\n",
      "Validation Loss: 737.5357\n",
      "Learning Rate: 0.000050\n",
      "------------------------------------------------------------\n",
      "Epoch 178\n",
      "Training Loss: 1375.8395\n",
      "Validation Loss: 717.4489\n",
      "Learning Rate: 0.000050\n",
      "------------------------------------------------------------\n",
      "Epoch 179\n",
      "Training Loss: 1361.1162\n",
      "Validation Loss: 721.5130\n",
      "Learning Rate: 0.000050\n",
      "------------------------------------------------------------\n",
      "Epoch 180\n",
      "Training Loss: 1360.9981\n",
      "Validation Loss: 708.4116\n",
      "Learning Rate: 0.000050\n",
      "------------------------------------------------------------\n",
      "Epoch 181\n",
      "Training Loss: 1344.6840\n",
      "Validation Loss: 728.0140\n",
      "Learning Rate: 0.000050\n",
      "------------------------------------------------------------\n",
      "Epoch 182\n",
      "Training Loss: 1350.2682\n",
      "Validation Loss: 706.5402\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 183\n",
      "Training Loss: 1302.7296\n",
      "Validation Loss: 669.5103\n",
      "Learning Rate: 0.000025\n",
      "Model saved at epoch 183\n",
      "------------------------------------------------------------\n",
      "Epoch 184\n",
      "Training Loss: 1293.4188\n",
      "Validation Loss: 675.3917\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 185\n",
      "Training Loss: 1280.3195\n",
      "Validation Loss: 664.3874\n",
      "Learning Rate: 0.000025\n",
      "Model saved at epoch 185\n",
      "------------------------------------------------------------\n",
      "Epoch 186\n",
      "Training Loss: 1265.0448\n",
      "Validation Loss: 663.7062\n",
      "Learning Rate: 0.000025\n",
      "Model saved at epoch 186\n",
      "------------------------------------------------------------\n",
      "Epoch 187\n",
      "Training Loss: 1278.9882\n",
      "Validation Loss: 668.4199\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 188\n",
      "Training Loss: 1265.4604\n",
      "Validation Loss: 671.1451\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 189\n",
      "Training Loss: 1256.0224\n",
      "Validation Loss: 669.8515\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 190\n",
      "Training Loss: 1264.1621\n",
      "Validation Loss: 660.5831\n",
      "Learning Rate: 0.000025\n",
      "Model saved at epoch 190\n",
      "------------------------------------------------------------\n",
      "Epoch 191\n",
      "Training Loss: 1247.9467\n",
      "Validation Loss: 657.4265\n",
      "Learning Rate: 0.000025\n",
      "Model saved at epoch 191\n",
      "------------------------------------------------------------\n",
      "Epoch 192\n",
      "Training Loss: 1268.8986\n",
      "Validation Loss: 652.8320\n",
      "Learning Rate: 0.000025\n",
      "Model saved at epoch 192\n",
      "------------------------------------------------------------\n",
      "Epoch 193\n",
      "Training Loss: 1249.4952\n",
      "Validation Loss: 656.1723\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 194\n",
      "Training Loss: 1232.7625\n",
      "Validation Loss: 656.6365\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 195\n",
      "Training Loss: 1225.1319\n",
      "Validation Loss: 648.5066\n",
      "Learning Rate: 0.000025\n",
      "Model saved at epoch 195\n",
      "------------------------------------------------------------\n",
      "Epoch 196\n",
      "Training Loss: 1243.0820\n",
      "Validation Loss: 655.7661\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 197\n",
      "Training Loss: 1228.1356\n",
      "Validation Loss: 664.5463\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 198\n",
      "Training Loss: 1225.3594\n",
      "Validation Loss: 641.2352\n",
      "Learning Rate: 0.000025\n",
      "Model saved at epoch 198\n",
      "------------------------------------------------------------\n",
      "Epoch 199\n",
      "Training Loss: 1234.3363\n",
      "Validation Loss: 644.6428\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 200\n",
      "Training Loss: 1223.2724\n",
      "Validation Loss: 639.5905\n",
      "Learning Rate: 0.000025\n",
      "Model saved at epoch 200\n",
      "------------------------------------------------------------\n",
      "Epoch 201\n",
      "Training Loss: 1214.7174\n",
      "Validation Loss: 644.7760\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 202\n",
      "Training Loss: 1237.9856\n",
      "Validation Loss: 644.4616\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 203\n",
      "Training Loss: 1228.4337\n",
      "Validation Loss: 645.1924\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 204\n",
      "Training Loss: 1227.8295\n",
      "Validation Loss: 640.0777\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 205\n",
      "Training Loss: 1193.6040\n",
      "Validation Loss: 639.4569\n",
      "Learning Rate: 0.000025\n",
      "Model saved at epoch 205\n",
      "------------------------------------------------------------\n",
      "Epoch 206\n",
      "Training Loss: 1201.7073\n",
      "Validation Loss: 634.0320\n",
      "Learning Rate: 0.000025\n",
      "Model saved at epoch 206\n",
      "------------------------------------------------------------\n",
      "Epoch 207\n",
      "Training Loss: 1209.7060\n",
      "Validation Loss: 643.7240\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 208\n",
      "Training Loss: 1199.2639\n",
      "Validation Loss: 631.5779\n",
      "Learning Rate: 0.000025\n",
      "Model saved at epoch 208\n",
      "------------------------------------------------------------\n",
      "Epoch 209\n",
      "Training Loss: 1213.0674\n",
      "Validation Loss: 639.5712\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 210\n",
      "Training Loss: 1205.4187\n",
      "Validation Loss: 635.5170\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 211\n",
      "Training Loss: 1197.3479\n",
      "Validation Loss: 631.1052\n",
      "Learning Rate: 0.000025\n",
      "Model saved at epoch 211\n",
      "------------------------------------------------------------\n",
      "Epoch 212\n",
      "Training Loss: 1211.2010\n",
      "Validation Loss: 630.5858\n",
      "Learning Rate: 0.000025\n",
      "Model saved at epoch 212\n",
      "------------------------------------------------------------\n",
      "Epoch 213\n",
      "Training Loss: 1186.6615\n",
      "Validation Loss: 628.2458\n",
      "Learning Rate: 0.000025\n",
      "Model saved at epoch 213\n",
      "------------------------------------------------------------\n",
      "Epoch 214\n",
      "Training Loss: 1191.0685\n",
      "Validation Loss: 631.3779\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 215\n",
      "Training Loss: 1206.2242\n",
      "Validation Loss: 633.6118\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 216\n",
      "Training Loss: 1185.5233\n",
      "Validation Loss: 643.5956\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 217\n",
      "Training Loss: 1203.7373\n",
      "Validation Loss: 622.4497\n",
      "Learning Rate: 0.000025\n",
      "Model saved at epoch 217\n",
      "------------------------------------------------------------\n",
      "Epoch 218\n",
      "Training Loss: 1193.5671\n",
      "Validation Loss: 624.3239\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 219\n",
      "Training Loss: 1174.3276\n",
      "Validation Loss: 632.2913\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 220\n",
      "Training Loss: 1195.9915\n",
      "Validation Loss: 617.9633\n",
      "Learning Rate: 0.000025\n",
      "Model saved at epoch 220\n",
      "------------------------------------------------------------\n",
      "Epoch 221\n",
      "Training Loss: 1179.7494\n",
      "Validation Loss: 620.0881\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 222\n",
      "Training Loss: 1179.9704\n",
      "Validation Loss: 619.1270\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 223\n",
      "Training Loss: 1192.7316\n",
      "Validation Loss: 620.7031\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 224\n",
      "Training Loss: 1187.4455\n",
      "Validation Loss: 632.6857\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 225\n",
      "Training Loss: 1179.8706\n",
      "Validation Loss: 617.3115\n",
      "Learning Rate: 0.000025\n",
      "Model saved at epoch 225\n",
      "------------------------------------------------------------\n",
      "Epoch 226\n",
      "Training Loss: 1174.9176\n",
      "Validation Loss: 617.6312\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 227\n",
      "Training Loss: 1160.3370\n",
      "Validation Loss: 619.3434\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 228\n",
      "Training Loss: 1168.3724\n",
      "Validation Loss: 610.2319\n",
      "Learning Rate: 0.000025\n",
      "Model saved at epoch 228\n",
      "------------------------------------------------------------\n",
      "Epoch 229\n",
      "Training Loss: 1176.2190\n",
      "Validation Loss: 610.4789\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 230\n",
      "Training Loss: 1164.6023\n",
      "Validation Loss: 617.6966\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 231\n",
      "Training Loss: 1179.7555\n",
      "Validation Loss: 609.0826\n",
      "Learning Rate: 0.000025\n",
      "Model saved at epoch 231\n",
      "------------------------------------------------------------\n",
      "Epoch 232\n",
      "Training Loss: 1171.4630\n",
      "Validation Loss: 607.8473\n",
      "Learning Rate: 0.000025\n",
      "Model saved at epoch 232\n",
      "------------------------------------------------------------\n",
      "Epoch 233\n",
      "Training Loss: 1155.3896\n",
      "Validation Loss: 616.1669\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 234\n",
      "Training Loss: 1163.5825\n",
      "Validation Loss: 603.2878\n",
      "Learning Rate: 0.000025\n",
      "Model saved at epoch 234\n",
      "------------------------------------------------------------\n",
      "Epoch 235\n",
      "Training Loss: 1160.6656\n",
      "Validation Loss: 612.8094\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 236\n",
      "Training Loss: 1178.9177\n",
      "Validation Loss: 609.8481\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 237\n",
      "Training Loss: 1159.0124\n",
      "Validation Loss: 607.8776\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 238\n",
      "Training Loss: 1167.2084\n",
      "Validation Loss: 607.2858\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 239\n",
      "Training Loss: 1164.1367\n",
      "Validation Loss: 609.8959\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 240\n",
      "Training Loss: 1166.6093\n",
      "Validation Loss: 602.5665\n",
      "Learning Rate: 0.000025\n",
      "Model saved at epoch 240\n",
      "------------------------------------------------------------\n",
      "Epoch 241\n",
      "Training Loss: 1140.5947\n",
      "Validation Loss: 597.4482\n",
      "Learning Rate: 0.000025\n",
      "Model saved at epoch 241\n",
      "------------------------------------------------------------\n",
      "Epoch 242\n",
      "Training Loss: 1149.4682\n",
      "Validation Loss: 604.4813\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 243\n",
      "Training Loss: 1153.3699\n",
      "Validation Loss: 606.3297\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 244\n",
      "Training Loss: 1142.7643\n",
      "Validation Loss: 604.4749\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 245\n",
      "Training Loss: 1136.4076\n",
      "Validation Loss: 595.8208\n",
      "Learning Rate: 0.000025\n",
      "Model saved at epoch 245\n",
      "------------------------------------------------------------\n",
      "Epoch 246\n",
      "Training Loss: 1154.7096\n",
      "Validation Loss: 605.9384\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 247\n",
      "Training Loss: 1159.1163\n",
      "Validation Loss: 598.5205\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 248\n",
      "Training Loss: 1156.3291\n",
      "Validation Loss: 596.8436\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 249\n",
      "Training Loss: 1131.5171\n",
      "Validation Loss: 596.3830\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 250\n",
      "Training Loss: 1137.5494\n",
      "Validation Loss: 601.5219\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 251\n",
      "Training Loss: 1132.9326\n",
      "Validation Loss: 592.5394\n",
      "Learning Rate: 0.000025\n",
      "Model saved at epoch 251\n",
      "------------------------------------------------------------\n",
      "Epoch 252\n",
      "Training Loss: 1131.9981\n",
      "Validation Loss: 593.9372\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 253\n",
      "Training Loss: 1135.1394\n",
      "Validation Loss: 588.2010\n",
      "Learning Rate: 0.000025\n",
      "Model saved at epoch 253\n",
      "------------------------------------------------------------\n",
      "Epoch 254\n",
      "Training Loss: 1127.0435\n",
      "Validation Loss: 591.3815\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 255\n",
      "Training Loss: 1125.9169\n",
      "Validation Loss: 585.2103\n",
      "Learning Rate: 0.000025\n",
      "Model saved at epoch 255\n",
      "------------------------------------------------------------\n",
      "Epoch 256\n",
      "Training Loss: 1124.4394\n",
      "Validation Loss: 583.4938\n",
      "Learning Rate: 0.000025\n",
      "Model saved at epoch 256\n",
      "------------------------------------------------------------\n",
      "Epoch 257\n",
      "Training Loss: 1128.6615\n",
      "Validation Loss: 582.1893\n",
      "Learning Rate: 0.000025\n",
      "Model saved at epoch 257\n",
      "------------------------------------------------------------\n",
      "Epoch 258\n",
      "Training Loss: 1119.8577\n",
      "Validation Loss: 585.1366\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 259\n",
      "Training Loss: 1118.6507\n",
      "Validation Loss: 588.7601\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 260\n",
      "Training Loss: 1128.9845\n",
      "Validation Loss: 589.2299\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 261\n",
      "Training Loss: 1121.2397\n",
      "Validation Loss: 582.0154\n",
      "Learning Rate: 0.000025\n",
      "Model saved at epoch 261\n",
      "------------------------------------------------------------\n",
      "Epoch 262\n",
      "Training Loss: 1116.0560\n",
      "Validation Loss: 584.7289\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 263\n",
      "Training Loss: 1106.5623\n",
      "Validation Loss: 585.3032\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 264\n",
      "Training Loss: 1118.1798\n",
      "Validation Loss: 588.5989\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 265\n",
      "Training Loss: 1106.4166\n",
      "Validation Loss: 588.0979\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 266\n",
      "Training Loss: 1113.3222\n",
      "Validation Loss: 579.2285\n",
      "Learning Rate: 0.000025\n",
      "Model saved at epoch 266\n",
      "------------------------------------------------------------\n",
      "Epoch 267\n",
      "Training Loss: 1113.4429\n",
      "Validation Loss: 581.5372\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 268\n",
      "Training Loss: 1112.0098\n",
      "Validation Loss: 570.9060\n",
      "Learning Rate: 0.000025\n",
      "Model saved at epoch 268\n",
      "------------------------------------------------------------\n",
      "Epoch 269\n",
      "Training Loss: 1123.0076\n",
      "Validation Loss: 576.9056\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 270\n",
      "Training Loss: 1099.5305\n",
      "Validation Loss: 570.5449\n",
      "Learning Rate: 0.000025\n",
      "Model saved at epoch 270\n",
      "------------------------------------------------------------\n",
      "Epoch 271\n",
      "Training Loss: 1093.0421\n",
      "Validation Loss: 585.2467\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 272\n",
      "Training Loss: 1114.5360\n",
      "Validation Loss: 580.5163\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 273\n",
      "Training Loss: 1109.6998\n",
      "Validation Loss: 569.7387\n",
      "Learning Rate: 0.000025\n",
      "Model saved at epoch 273\n",
      "------------------------------------------------------------\n",
      "Epoch 274\n",
      "Training Loss: 1096.3926\n",
      "Validation Loss: 569.2066\n",
      "Learning Rate: 0.000025\n",
      "Model saved at epoch 274\n",
      "------------------------------------------------------------\n",
      "Epoch 275\n",
      "Training Loss: 1107.7489\n",
      "Validation Loss: 574.8612\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 276\n",
      "Training Loss: 1115.6394\n",
      "Validation Loss: 573.3907\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 277\n",
      "Training Loss: 1104.7095\n",
      "Validation Loss: 572.4245\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 278\n",
      "Training Loss: 1099.8952\n",
      "Validation Loss: 571.3009\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 279\n",
      "Training Loss: 1088.6460\n",
      "Validation Loss: 566.8602\n",
      "Learning Rate: 0.000025\n",
      "Model saved at epoch 279\n",
      "------------------------------------------------------------\n",
      "Epoch 280\n",
      "Training Loss: 1091.7360\n",
      "Validation Loss: 571.8363\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 281\n",
      "Training Loss: 1084.2336\n",
      "Validation Loss: 570.7207\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 282\n",
      "Training Loss: 1098.3353\n",
      "Validation Loss: 577.6168\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 283\n",
      "Training Loss: 1094.4932\n",
      "Validation Loss: 571.3736\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 284\n",
      "Training Loss: 1088.4864\n",
      "Validation Loss: 561.0103\n",
      "Learning Rate: 0.000025\n",
      "Model saved at epoch 284\n",
      "------------------------------------------------------------\n",
      "Epoch 285\n",
      "Training Loss: 1108.5919\n",
      "Validation Loss: 563.8266\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 286\n",
      "Training Loss: 1084.0695\n",
      "Validation Loss: 571.3834\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 287\n",
      "Training Loss: 1082.1392\n",
      "Validation Loss: 568.8370\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 288\n",
      "Training Loss: 1085.2698\n",
      "Validation Loss: 567.0240\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 289\n",
      "Training Loss: 1102.1399\n",
      "Validation Loss: 568.9365\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 290\n",
      "Training Loss: 1081.7135\n",
      "Validation Loss: 556.5644\n",
      "Learning Rate: 0.000025\n",
      "Model saved at epoch 290\n",
      "------------------------------------------------------------\n",
      "Epoch 291\n",
      "Training Loss: 1086.3531\n",
      "Validation Loss: 558.0782\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 292\n",
      "Training Loss: 1086.1476\n",
      "Validation Loss: 569.3242\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 293\n",
      "Training Loss: 1065.4342\n",
      "Validation Loss: 562.0585\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 294\n",
      "Training Loss: 1079.9447\n",
      "Validation Loss: 557.2762\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 295\n",
      "Training Loss: 1075.6481\n",
      "Validation Loss: 557.6405\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 296\n",
      "Training Loss: 1068.8679\n",
      "Validation Loss: 552.3491\n",
      "Learning Rate: 0.000025\n",
      "Model saved at epoch 296\n",
      "------------------------------------------------------------\n",
      "Epoch 297\n",
      "Training Loss: 1066.9148\n",
      "Validation Loss: 552.9297\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 298\n",
      "Training Loss: 1077.0256\n",
      "Validation Loss: 552.3238\n",
      "Learning Rate: 0.000025\n",
      "Model saved at epoch 298\n",
      "------------------------------------------------------------\n",
      "Epoch 299\n",
      "Training Loss: 1060.3179\n",
      "Validation Loss: 555.2270\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Epoch 300\n",
      "Training Loss: 1087.1064\n",
      "Validation Loss: 553.9816\n",
      "Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # # データの読み込み\n",
    "    # data_pairs = [\n",
    "    #     #\n",
    "    #     # 第三回収集データ\n",
    "    #     #\n",
    "    #     # # 立ちっぱなし\n",
    "    #     ('./data/20250517old_data/20241115test3/Opti-track/Take 2024-11-15 03.20.00 PM.csv',\n",
    "    #      './data/20250517old_data/20241115test3/insoleSensor/20241115_152500_left.csv',\n",
    "    #      './data/20250517old_data/20241115test3/insoleSensor/20241115_152500_right.csv'),\n",
    "    #     # お辞儀\n",
    "    #     ('./data/20250517old_data/20241115test3/Opti-track/Take 2024-11-15 03.26.00 PM.csv',\n",
    "    #      './data/20250517old_data/20241115test3/insoleSensor/20241115_153100_left.csv',\n",
    "    #      './data/20250517old_data/20241115test3/insoleSensor/20241115_153100_right.csv'),\n",
    "    #     # 体の横の傾け\n",
    "    #     ('./data/20250517old_data/20241115test3/Opti-track/Take 2024-11-15 03.32.00 PM.csv',\n",
    "    #      './data/20250517old_data/20241115test3/insoleSensor/20241115_153700_left.csv',\n",
    "    #      './data/20250517old_data/20241115test3/insoleSensor/20241115_153700_right.csv'),\n",
    "    #     # 立つ座る\n",
    "    #     ('./data/20250517old_data/20241115test3/Opti-track/Take 2024-11-15 03.38.00 PM.csv',\n",
    "    #      './data/20250517old_data/20241115test3/insoleSensor/20241115_154300_left.csv',\n",
    "    #      './data/20250517old_data/20241115test3/insoleSensor/20241115_154300_right.csv'),\n",
    "    #     # スクワット\n",
    "    #     ('./data/20250517old_data/20241115test3/Opti-track/Take 2024-11-15 03.44.00 PM.csv',\n",
    "    #      './data/20250517old_data/20241115test3/insoleSensor/20241115_154900_left.csv',\n",
    "    #      './data/20250517old_data/20241115test3/insoleSensor/20241115_154900_right.csv'),\n",
    "    #     # 総合(test3)\n",
    "    #     # ('./data/20241115test3/Opti-track/Take 2024-11-15 03.50.00 PM.csv',\n",
    "    #     # './data/20241115test3/insoleSensor/20241115_155500_left.csv', \n",
    "    #     # './data/20241115test3/insoleSensor/20241115_155500_right.csv'),\n",
    "\n",
    "    #     # 釘宮くん\n",
    "    #     ('./data/20250517old_data/20241212test4/Opti-track/Take 2024-12-12 03.06.59 PM.csv',\n",
    "    #      './data/20250517old_data/20241212test4/insoleSensor/20241212_152700_left.csv',\n",
    "    #      './data/20250517old_data/20241212test4/insoleSensor/20241212_152700_right.csv'),\n",
    "    #     # 百田くん\n",
    "    #     ('./data/20250517old_data/20241212test4/Opti-track/Take 2024-12-12 03.45.00 PM.csv',\n",
    "    #      './data/20250517old_data/20241212test4/insoleSensor/20241212_160501_left.csv',\n",
    "    #      './data/20250517old_data/20241212test4/insoleSensor/20241212_160501_right.csv'),\n",
    "    #     # # # # 渡辺(me)\n",
    "    #     ('./data/20250517old_data/20241212test4/Opti-track/Take 2024-12-12 04.28.00 PM.csv',\n",
    "    #      './data/20250517old_data/20241212test4/insoleSensor/20241212_164800_left.csv',\n",
    "    #      './data/20250517old_data/20241212test4/insoleSensor/20241212_164800_right.csv'),\n",
    "    #     # にるぱむさん\n",
    "    #     ('./data/20250517old_data/20241212test4/Opti-track/Take 2024-12-12 05.17.59 PM.csv',\n",
    "    #      './data/20250517old_data/20241212test4/insoleSensor/20241212_173800_left.csv',\n",
    "    #      './data/20250517old_data/20241212test4/insoleSensor/20241212_173800_right.csv')\n",
    "    # ]\n",
    "\n",
    "    # トレーニングデータ\n",
    "    data_pairs = [\n",
    "        ('./data/20250518test3/Opti-track/3_final/Take 2024-11-15 03.19.59 PM.csv',\n",
    "        './data/20250518test3/insoleSensor/3_final/20241115_152500_left.csv',\n",
    "         './data/20250518test3/insoleSensor/3_final/20241115_152500_right.csv'),\n",
    "        ('./data/20250518test3/Opti-track/3_final/Take 2024-11-15 03.26.00 PM.csv',\n",
    "         './data/20250518test3/insoleSensor/3_final/20241115_153100_left.csv', \n",
    "         './data/20250518test3/insoleSensor/3_final/20241115_153100_right.csv'),\n",
    "        ('./data/20250518test3/Opti-track/3_final/Take 2024-11-15 03.31.59 PM.csv', \n",
    "         './data/20250518test3/insoleSensor/3_final/20241115_153700_left.csv', \n",
    "         './data/20250518test3/insoleSensor/3_final/20241115_153700_right.csv'),\n",
    "        ('./data/20250518test3/Opti-track/3_final/Take 2024-11-15 03.37.59 PM.csv', \n",
    "         './data/20250518test3/insoleSensor/3_final/20241115_154300_left.csv', \n",
    "         './data/20250518test3/insoleSensor/3_final/20241115_154300_right.csv'),\n",
    "        ('./data/20250518test3/Opti-track/3_final/Take 2024-11-15 03.43.59 PM.csv', \n",
    "         './data/20250518test3/insoleSensor/3_final/20241115_154900_left.csv', \n",
    "         './data/20250518test3/insoleSensor/3_final/20241115_154900_right.csv'),\n",
    "         './data/20250518test4/insoleSensor/3_final/20241212_152700_left.csv', \n",
    "        ('./data/20250518test4/Opti-track/3_final/Take 2024-12-12 03.06.59 PM.csv',\n",
    "         './data/20250518test4/insoleSensor/3_final/20241212_152700_right.csv'),\n",
    "        ('./data/20250518test4/Opti-track/3_final/Take 2024-12-12 03.45.00 PM.csv', \n",
    "         './data/20250518test4/insoleSensor/3_final/20241212_160501_left.csv', \n",
    "         './data/20250518test4/insoleSensor/3_final/20241212_160501_right.csv'),\n",
    "        ('./data/20250518test4/Opti-track/3_final/Take 2024-12-12 04.28.00 PM.csv', \n",
    "         './data/20250518test4/insoleSensor/3_final/20241212_164800_left.csv', \n",
    "         './data/20250518test4/insoleSensor/3_final/20241212_164800_right.csv'),\n",
    "        ('./data/20250518test4/Opti-track/3_final/Take 2024-12-12 05.17.59 PM.csv', \n",
    "         './data/20250518test4/insoleSensor/3_final/20241212_173800_left.csv', \n",
    "         './data/20250518test4/insoleSensor/3_final/20241212_173800_right.csv')\n",
    "]\n",
    "\n",
    "    # データの読み込みと結合\n",
    "    seq_length = 3\n",
    "    window_size = 1\n",
    "    skeleton_data, pressure_data_left, pressure_data_right, decoder_input, skeleton_label = load_and_combine_data(\n",
    "        data_pairs, seq_length, window_size)\n",
    "\n",
    "    # numpy配列に変換\n",
    "    skeleton_data = skeleton_data.to_numpy()\n",
    "    decoder_input = decoder_input\n",
    "\n",
    "    # 圧力、回転、加速度データの前処理\n",
    "    input_features, sensor_scalers = preprocess_pressure_data(\n",
    "        pressure_data_left,\n",
    "        pressure_data_right\n",
    "    )\n",
    "    print(input_features.shape)\n",
    "\n",
    "    # データの分割\n",
    "    train_input, val_input, train_skeleton, val_skeleton, train_decoder_input, val_decoder_input = train_test_split(\n",
    "        input_features,\n",
    "        skeleton_label,\n",
    "        decoder_input,\n",
    "        test_size=0.2,\n",
    "        random_state=42\n",
    "    )\n",
    "    print(train_decoder_input.shape)\n",
    "    print(val_decoder_input.shape)\n",
    "\n",
    "    print(train_decoder_input[0])\n",
    "    # デバイスの設定\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # モデルのパラメータ設定\n",
    "    input_dim = input_features.shape[1]  # 圧力+回転+加速度の合計次元数\n",
    "    d_model = 512\n",
    "    nhead = 8\n",
    "    num_encoder_layers = 6\n",
    "    num_joints = 21 # skeleton_data.shape[1] // 3  # 3D座標なので3で割る\n",
    "    dropout = 0.1\n",
    "    batch_size = 128\n",
    "\n",
    "    # データローダーの設定\n",
    "    train_dataset = PressureSkeletonDataset(train_input, train_skeleton, train_decoder_input)\n",
    "    val_dataset = PressureSkeletonDataset(val_input, val_skeleton, val_decoder_input)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    print(\"Checking final training and validation data...\")\n",
    "    print(\"Train input NaN count:\", np.isnan(train_input).sum(), \"Inf count:\", np.isinf(train_input).sum())\n",
    "    print(\"Train skeleton NaN count:\", np.isnan(train_skeleton).sum(), \"Inf count:\", np.isinf(train_skeleton).sum())\n",
    "\n",
    "    # モデルの初期化\n",
    "    model = EnhancedSkeletonTransformer(\n",
    "        input_dim=input_features.shape[1],  # input_dim,\n",
    "        d_model=d_model,\n",
    "        nhead=nhead,\n",
    "        num_encoder_layers=num_encoder_layers,\n",
    "        num_joints=num_joints,\n",
    "        num_dims=3,\n",
    "        dropout=dropout,\n",
    "        seq_length=seq_length,\n",
    "        window_size=window_size\n",
    "    ).to(device)\n",
    "\n",
    "    # 損失関数、オプティマイザ、スケジューラの設定\n",
    "    # criterion = torch.nn.MSELoss()  # 必要に応じてカスタム損失関数に変更可能\n",
    "    criterion = EnhancedSkeletonLoss_WithAngleConstrains(alpha=1.0, beta=0.1, gamma=0.5, window_size=window_size)\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=0.0001,\n",
    "        weight_decay=0.001,\n",
    "        betas=(0.9, 0.999)\n",
    "    )\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        mode='min',\n",
    "        factor=0.5,\n",
    "        patience=5\n",
    "    )\n",
    "\n",
    "    # トレーニング実行\n",
    "    train_model(\n",
    "        model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        criterion,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        num_epochs=300,\n",
    "        save_path='./weight/best_skeleton_model.pth',\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    # モデルの保存\n",
    "    final_checkpoint = {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'sensor_scalers': sensor_scalers,\n",
    "        # 'skeleton_skaler': skeleton_scaler,\n",
    "        'model_config': {\n",
    "            'input_dim': input_dim,\n",
    "            'd_model': d_model,\n",
    "            'nhead': nhead,\n",
    "            'num_encoder_layers': num_encoder_layers,\n",
    "            'num_joints': num_joints\n",
    "        }\n",
    "    }\n",
    "    torch.save(final_checkpoint, './weight/final_skeleton_model.pth')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf21fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting prediction process...\n",
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\20250415P2P-Insole\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:382: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully\n",
      "torch.Size([3004, 1, 63])\n",
      "Making predictions...\n",
      "Prediction shape: torch.Size([2994, 63])\n",
      "Loss: 1.7209700345993042\n",
      "Loss: 20846.263671875\n",
      "\n",
      "Saving predictions...\n",
      "Predictions saved to ./output/predicted_skeleton.csv\n",
      "[[ 6.1431308e+00  8.7767566e+02  1.5188855e+00 ... -8.3849739e+01\n",
      "   8.5266762e+01 -1.0153264e+02]\n",
      " [ 5.6159401e+00  8.7081726e+02  1.8512084e+00 ... -7.2973282e+01\n",
      "   7.8227470e+01 -9.4986206e+01]\n",
      " [ 5.1593361e+00  8.7464093e+02  2.1297028e+00 ... -7.0769012e+01\n",
      "   8.3465721e+01 -9.1293549e+01]\n",
      " ...\n",
      " [ 7.3100266e+00  8.7740967e+02 -6.0823417e-01 ... -1.0187366e+02\n",
      "   8.5396461e+01 -1.6579375e+02]\n",
      " [ 7.2772064e+00  8.7744037e+02 -5.6451070e-01 ... -1.0103913e+02\n",
      "   8.5198021e+01 -1.6396756e+02]\n",
      " [ 7.2693353e+00  8.7748822e+02 -4.3247700e-01 ... -1.0097265e+02\n",
      "   8.5110474e+01 -1.6256560e+02]]\n",
      "Prediction process completed successfully!\n"
     ]
    }
   ],
   "source": [
    "def preprocess_pressure_data(left_data, right_data):\n",
    "    \"\"\"圧力、回転、加速度データの前処理\"\"\"\n",
    "    # 左足データから各種センサー値を抽出\n",
    "    left_pressure = left_data.iloc[:, :35]\n",
    "    left_rotation = left_data.iloc[:, 35:38]\n",
    "    left_accel = left_data.iloc[:, 38:41]\n",
    "\n",
    "    # 右足データから各種センサー値を抽出\n",
    "    right_pressure = right_data.iloc[:, :35]\n",
    "    right_rotation = right_data.iloc[:, 35:38]\n",
    "    right_accel = right_data.iloc[:, 38:41]\n",
    "\n",
    "    # データの結合\n",
    "    pressure_combined = pd.concat([left_pressure, right_pressure], axis=1)\n",
    "    rotation_combined = pd.concat([left_rotation, right_rotation], axis=1)\n",
    "    accel_combined = pd.concat([left_accel, right_accel], axis=1)\n",
    "\n",
    "    # NaN値を補正\n",
    "    pressure_combined = pressure_combined.ffill().bfill()\n",
    "    rotation_combined = rotation_combined.ffill().bfill()\n",
    "    accel_combined = accel_combined.ffill().bfill()\n",
    "\n",
    "    # 移動平均フィルタの適用\n",
    "    window_size = 3\n",
    "    pressure_combined = pressure_combined.rolling(window=window_size, center=True).mean()\n",
    "    rotation_combined = rotation_combined.rolling(window=window_size, center=True).mean()\n",
    "    accel_combined = accel_combined.rolling(window=window_size, center=True).mean()\n",
    "\n",
    "    # NaN値を補間\n",
    "    pressure_combined = pressure_combined.ffill().bfill()\n",
    "    rotation_combined = rotation_combined.ffill().bfill()\n",
    "    accel_combined = accel_combined.ffill().bfill()\n",
    "\n",
    "    # 正規化と標準化\n",
    "    pressure_normalizer = MinMaxScaler()\n",
    "    rotation_normalizer = MinMaxScaler()\n",
    "    accel_normalizer = MinMaxScaler()\n",
    "\n",
    "    pressure_standardizer = StandardScaler(with_mean=True, with_std=True)\n",
    "    rotation_standardizer = StandardScaler(with_mean=True, with_std=True)\n",
    "    accel_standardizer = StandardScaler(with_mean=True, with_std=True)\n",
    "\n",
    "    # データの正規化と標準化\n",
    "    pressure_processed = pressure_standardizer.fit_transform(\n",
    "        pressure_normalizer.fit_transform(pressure_combined)\n",
    "    )\n",
    "    rotation_processed = rotation_standardizer.fit_transform(\n",
    "        rotation_normalizer.fit_transform(rotation_combined)\n",
    "    )\n",
    "    accel_processed = accel_standardizer.fit_transform(\n",
    "        accel_normalizer.fit_transform(accel_combined)\n",
    "    )\n",
    "\n",
    "    # 1次微分と2次微分の計算\n",
    "    pressure_grad1 = np.gradient(pressure_processed, axis=0)\n",
    "    pressure_grad2 = np.gradient(pressure_grad1, axis=0)\n",
    "\n",
    "    rotation_grad1 = np.gradient(rotation_processed, axis=0)\n",
    "    rotation_grad2 = np.gradient(rotation_grad1, axis=0)\n",
    "\n",
    "    accel_grad1 = np.gradient(accel_processed, axis=0)\n",
    "    accel_grad2 = np.gradient(accel_grad1, axis=0)\n",
    "\n",
    "    # すべての特徴量を結合（246次元になるはず）\n",
    "    input_features = np.concatenate([\n",
    "        pressure_processed,  # 原特徴量\n",
    "        # pressure_grad1,     # 1次微分\n",
    "        # pressure_grad2,     # 2次微分\n",
    "        rotation_processed,\n",
    "        # rotation_grad1,\n",
    "        # rotation_grad2,\n",
    "        accel_processed,\n",
    "        # accel_grad1,\n",
    "        # accel_grad2\n",
    "    ], axis=1)\n",
    "\n",
    "    return input_features\n",
    "\n",
    "\n",
    "def load_and_preprocess_data(file_pairs):\n",
    "    predictions_all = []\n",
    "\n",
    "    for skeleton_file, left_file, right_file in file_pairs:\n",
    "        skeleton_data = pd.read_csv(skeleton_file)\n",
    "        pressure_data_left = pd.read_csv(left_file)\n",
    "        pressure_data_right = pd.read_csv(right_file)\n",
    "\n",
    "        input_features = preprocess_pressure_data(pressure_data_left, pressure_data_right)\n",
    "        min_length = min(len(skeleton_data), len(input_features))\n",
    "\n",
    "        input_features = input_features.iloc[:min_length]\n",
    "        skeleton_data = skeleton_data.iloc[:min_length]\n",
    "\n",
    "        predictions_all.append((input_features, skeleton_data))\n",
    "\n",
    "    return predictions_all\n",
    "\n",
    "\n",
    "def predict_skeleton():\n",
    "    try:\n",
    "        # # データの読み込みと前処理\n",
    "        # skeleton_data = pd.read_csv('./data/20250517old_data/20241115test3/Opti-track/Take 2024-11-15 03.50.00 PM.csv')\n",
    "        # pressure_data_left = pd.read_csv('./data/20250517old_data/20241115test3/insoleSensor/20241115_155500_left.csv', skiprows=1)\n",
    "        # pressure_data_right = pd.read_csv('./data/20250517old_data/20241115test3/insoleSensor/20241115_155500_right.csv', skiprows=1)\n",
    "\n",
    "        # Newテストデータ\n",
    "        skeleton_data = pd.read_csv('./data/20250518test3/Opti-track/3_final/Take 2024-11-15 03.49.59 PM.csv') \n",
    "        pressure_data_left = pd.read_csv('./data/20250518test3/insoleSensor/3_final/20241115_155500_left.csv') \n",
    "        pressure_data_right = pd.read_csv('./data/20250518test3/insoleSensor/3_final/20241115_155500_right.csv')\n",
    "\n",
    "        # 入力データの前処理\n",
    "        input_features = preprocess_pressure_data(pressure_data_left, pressure_data_right)\n",
    "        min_length = min(input_features.shape[0], skeleton_data.shape[0])\n",
    "\n",
    "        # 入力の次元数を取得\n",
    "        seq_length = 3\n",
    "        window_size = 1\n",
    "        m = 1\n",
    "        input_dim = input_features.shape[1]\n",
    "        num_joints = skeleton_data.shape[1] // 3\n",
    "\n",
    "        # デバイスの設定\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"Using device: {device}\")\n",
    "        skeleton_data = torch.FloatTensor(np.array(skeleton_data)[:min_length]).to(device)\n",
    "\n",
    "        # モデルの初期化（固定パラメータを使用）\n",
    "        model = EnhancedSkeletonTransformer(\n",
    "            input_dim=input_dim,\n",
    "            d_model=512,\n",
    "            nhead=8,\n",
    "            num_encoder_layers=6,\n",
    "            num_joints=num_joints,\n",
    "            num_dims=3,\n",
    "            dropout=0.1,\n",
    "            seq_length=seq_length,\n",
    "            window_size=window_size\n",
    "        ).to(device)\n",
    "\n",
    "        # チェックポイントの読み込み（weights_only=Trueを追加）\n",
    "        checkpoint = torch.load('./weight/best_skeleton_model.pth', map_location=device, weights_only=True)\n",
    "\n",
    "        # モデルの重みを読み込み\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        model.eval()\n",
    "        print(\"Model loaded successfully\")\n",
    "\n",
    "        action = torch.zeros((min_length + 10, window_size, 63)).to(device)\n",
    "        num = np.zeros(min_length + 10)\n",
    "        print(action.shape)\n",
    "        # 予測の実行\n",
    "        print(\"Making predictions...\")\n",
    "        predictions = torch.zeros(min_length, 63).to(device)\n",
    "        with torch.no_grad():\n",
    "            skeleton_last = torch.zeros((seq_length, 63))\n",
    "            skeleton_last = skeleton_last.unsqueeze(0).to(device)\n",
    "            for i in range(min_length):\n",
    "                input_tensor = torch.FloatTensor(input_features)[i].to(device)\n",
    "                input_tensor = input_tensor.unsqueeze(0).to(device)\n",
    "                # if i%200==0:\n",
    "                #    skeleton_last=torch.zeros_like(skeleton_last)\n",
    "\n",
    "                skeleton_predict_seq = model(input_tensor, skeleton_last)\n",
    "                skeleton_predict_seq = skeleton_predict_seq.squeeze(0)\n",
    "                skeleton_predict = torch.zeros(63).to(device)\n",
    "                for j in range(window_size):\n",
    "                    action[i + j, int(num[i + j])] = skeleton_predict_seq[j, :]\n",
    "                    num[i + j] += 1\n",
    "                    # print(f\"j={j},i+j={i+j},num[i+j}]={int(num[i+j])}\")\n",
    "                weights = compute_exponential_weights(int(num[i]), m).to(device)\n",
    "                for j in range(int(num[i])):\n",
    "                    skeleton_predict += weights[j] * action[i, int(num[i]) - 1 - j]\n",
    "                predictions[i] = skeleton_predict\n",
    "                for j in range(seq_length - 1):\n",
    "                    skeleton_last[0, j] = skeleton_last[0, j + 1]\n",
    "                skeleton_last[0, seq_length - 1] = skeleton_predict\n",
    "        '''\n",
    "        # 予測の実行\n",
    "        print(\"Making predictions...\")\n",
    "        predictions=torch.zeros(min_length,63).to(device)\n",
    "        with torch.no_grad():\n",
    "            input_tensor = torch.FloatTensor(input_features).to(device)\n",
    "            input_tensor=input_tensor.to(device)\n",
    "            print(input_tensor.shape,skeleton_data.shape)\n",
    "            predictions=model(input_tensor,skeleton_data)\n",
    "        '''\n",
    "        print(f\"Prediction shape: {predictions.shape}\")\n",
    "        criterion = EnhancedSkeletonLoss(alpha=1.0, beta=0.1, gamma=0.1, window_size=1)\n",
    "        criterion1 = EnhancedSkeletonLoss_WithAngleConstrains(alpha=1.0, beta=0.1, gamma=0.1, window_size=1)\n",
    "        predictionsa = predictions.unsqueeze(1)\n",
    "        skeleton_dataa = skeleton_data.unsqueeze(1)\n",
    "        loss = criterion(predictionsa, skeleton_dataa)\n",
    "        print(f\"Loss: {loss}\")\n",
    "        loss1 = criterion1(predictionsa, skeleton_dataa)\n",
    "        print(f\"Loss: {loss1}\")\n",
    "        predictions = predictions.cpu().numpy()\n",
    "        return predictions\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during prediction: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def save_predictions(predictions, output_file='./output/predicted_skeleton.csv'):\n",
    "    try:\n",
    "        # 予測結果をデータフレームに変換\n",
    "        num_joints = predictions.shape[1] // 3\n",
    "        columns = []\n",
    "        for i in range(num_joints):\n",
    "            columns.extend([f'X.{i * 2 + 1}', f'Y.{i * 2 + 1}', f'Z.{i * 2 + 1}'])\n",
    "\n",
    "        df_predictions = pd.DataFrame(predictions, columns=columns)\n",
    "        df_predictions.to_csv(output_file, index=False)\n",
    "        print(f\"Predictions saved to {output_file}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving predictions: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        print(\"Starting prediction process...\")\n",
    "        predictions = predict_skeleton()\n",
    "\n",
    "        print(\"\\nSaving predictions...\")\n",
    "        save_predictions(predictions)\n",
    "        print(predictions)\n",
    "\n",
    "        print(\"Prediction process completed successfully!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
